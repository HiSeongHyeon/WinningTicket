{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 모델이 Input Size가 2048이라면 4096개 중에서 앞의 2048만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 0. Data_Prepare 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV already exists for file: B_007_0.mat, skipping creation.\n",
      "CSV already exists for file: B_007_1.mat, skipping creation.\n",
      "CSV already exists for file: B_007_2.mat, skipping creation.\n",
      "CSV already exists for file: B_007_3.mat, skipping creation.\n",
      "CSV already exists for file: B_014_0.mat, skipping creation.\n",
      "CSV already exists for file: B_014_1.mat, skipping creation.\n",
      "CSV already exists for file: B_014_2.mat, skipping creation.\n",
      "CSV already exists for file: B_014_3.mat, skipping creation.\n",
      "CSV already exists for file: B_021_0.mat, skipping creation.\n",
      "CSV already exists for file: B_021_1.mat, skipping creation.\n",
      "CSV already exists for file: B_021_2.mat, skipping creation.\n",
      "CSV already exists for file: B_021_3.mat, skipping creation.\n",
      "CSV already exists for file: IR_007_0.mat, skipping creation.\n",
      "CSV already exists for file: IR_007_1.mat, skipping creation.\n",
      "CSV already exists for file: IR_007_2.mat, skipping creation.\n",
      "CSV already exists for file: IR_007_3.mat, skipping creation.\n",
      "CSV already exists for file: IR_014_0.mat, skipping creation.\n",
      "CSV already exists for file: IR_014_1.mat, skipping creation.\n",
      "CSV already exists for file: IR_014_2.mat, skipping creation.\n",
      "CSV already exists for file: IR_014_3.mat, skipping creation.\n",
      "CSV already exists for file: IR_021_0.mat, skipping creation.\n",
      "CSV already exists for file: IR_021_1.mat, skipping creation.\n",
      "CSV already exists for file: IR_021_2.mat, skipping creation.\n",
      "CSV already exists for file: IR_021_3.mat, skipping creation.\n",
      "CSV already exists for file: N_000_0.mat, skipping creation.\n",
      "CSV already exists for file: N_000_1.mat, skipping creation.\n",
      "CSV already exists for file: N_000_2.mat, skipping creation.\n",
      "CSV already exists for file: N_000_3.mat, skipping creation.\n",
      "Unknown fault type in file name: OR@03_007_0.mat\n",
      "Unknown fault type in file name: OR@03_007_1.mat\n",
      "Unknown fault type in file name: OR@03_007_2.mat\n",
      "Unknown fault type in file name: OR@03_007_3.mat\n",
      "Unknown fault type in file name: OR@03_021_0.mat\n",
      "Unknown fault type in file name: OR@03_021_1.mat\n",
      "Unknown fault type in file name: OR@03_021_2.mat\n",
      "Unknown fault type in file name: OR@03_021_3.mat\n",
      "CSV already exists for file: OR@06_007_0.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_007_1.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_007_2.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_007_3.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_014_0.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_014_1.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_014_2.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_014_3.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_021_0.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_021_1.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_021_2.mat, skipping creation.\n",
      "CSV already exists for file: OR@06_021_3.mat, skipping creation.\n",
      "Unknown fault type in file name: OR@12_007_0.mat\n",
      "Unknown fault type in file name: OR@12_007_1.mat\n",
      "Unknown fault type in file name: OR@12_007_2.mat\n",
      "Unknown fault type in file name: OR@12_007_3.mat\n",
      "Unknown fault type in file name: OR@12_021_0.mat\n",
      "Unknown fault type in file name: OR@12_021_1.mat\n",
      "Unknown fault type in file name: OR@12_021_2.mat\n",
      "Unknown fault type in file name: OR@12_021_3.mat\n"
     ]
    }
   ],
   "source": [
    "def Data_Prepare():\n",
    "    current_dir = os.getcwd()\n",
    "    base_path = os.path.join(current_dir, \"..\", \"..\", \"..\", \"Dataset\", \"CWRU\", \"12k\")\n",
    "    save_path = os.path.join(current_dir, \"..\", \"..\", \"..\", \"Dataset\", \"CWRU\", \"Data Preprocessing(DE)\")\n",
    "\n",
    "    fault_types = ['N', 'B', 'IR', 'OR@06']\n",
    "\n",
    "    # Create directories for Data Preprocessing\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for fault in fault_types:\n",
    "        os.makedirs(os.path.join(save_path, fault), exist_ok=True)\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    try:\n",
    "        if not os.path.exists(base_path):\n",
    "            raise FileNotFoundError(f\"Base path does not exist: {base_path}\")\n",
    "\n",
    "        for file in os.listdir(base_path):\n",
    "            if file.endswith('.mat') and '_028' not in file:\n",
    "                file_path = os.path.join(base_path, file)\n",
    "\n",
    "                # Load MATLAB file\n",
    "                mat_data = sio.loadmat(file_path)\n",
    "\n",
    "                # Extract DE (Drive End) data (dynamic key detection for DE_time)\n",
    "                de_key = next((key for key in mat_data.keys() if \"DE_time\" in key), None)\n",
    "                if de_key:\n",
    "                    de_data = mat_data[de_key].flatten()\n",
    "                else:\n",
    "                    print(f\"No DE_time data found in file: {file}\")\n",
    "                    continue\n",
    "\n",
    "                # Determine fault type from file name\n",
    "                fault_type = file.split('_')[0]\n",
    "                if fault_type not in fault_types:\n",
    "                    print(f\"Unknown fault type in file name: {file}\")\n",
    "                    continue\n",
    "\n",
    "                # Save DE data as CSV without header\n",
    "                fault_save_path = os.path.join(save_path, fault_type)\n",
    "                csv_save_path = os.path.join(fault_save_path, f\"{file.replace('_DE', '')[:-4]}.csv\")\n",
    "                if not os.path.exists(csv_save_path):\n",
    "                    pd.DataFrame(de_data).to_csv(csv_save_path, index=False, header=False)\n",
    "                    print(f\"Processed and saved DE data for file: {file}\")\n",
    "                else:\n",
    "                    print(f\"CSV already exists for file: {file}, skipping creation.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "Data_Prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .mat 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN, Created on: Mon Jan 31 13:59:43 2000', '__version__': '1.0', '__globals__': [], 'X130_DE_time': array([[ 0.00852784],\n",
      "       [ 0.4235496 ],\n",
      "       [ 0.01299481],\n",
      "       ...,\n",
      "       [ 0.17583603],\n",
      "       [ 0.1100498 ],\n",
      "       [-0.10274022]]), 'X130_FE_time': array([[-0.40700545],\n",
      "       [ 0.26277636],\n",
      "       [ 0.49514545],\n",
      "       ...,\n",
      "       [ 0.05958182],\n",
      "       [-0.05074727],\n",
      "       [ 0.02732545]]), 'X130_BA_time': array([[-4.02373887e-05],\n",
      "       [ 6.93290208e-02],\n",
      "       [ 3.06608902e-02],\n",
      "       ...,\n",
      "       [ 1.18297923e-01],\n",
      "       [ 5.66542433e-02],\n",
      "       [-1.05019585e-02]]), 'X130RPM': array([[1796]], dtype=uint16)}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "\n",
    "# Set the base path and file name\n",
    "current_dir = os.getcwd()\n",
    "base_path = os.path.join(current_dir, \"..\", \"..\", \"..\", \"Dataset\", \"CWRU\", \"12k\")\n",
    "file_name = \"OR@06_007_0.mat\"\n",
    "file_path = os.path.join(base_path, file_name)\n",
    "\n",
    "mat_data = sio.loadmat(file_path)\n",
    "print(mat_data)\n",
    "print(type(mat_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1. Data 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved samples for file: N_000_0.csv\n",
      "Processed and saved samples for file: N_000_1.csv\n",
      "Processed and saved samples for file: N_000_2.csv\n",
      "Processed and saved samples for file: N_000_3.csv\n",
      "Processed and saved samples for file: B_007_0.csv\n",
      "Processed and saved samples for file: B_007_1.csv\n",
      "Processed and saved samples for file: B_007_2.csv\n",
      "Processed and saved samples for file: B_007_3.csv\n",
      "Processed and saved samples for file: B_014_0.csv\n",
      "Processed and saved samples for file: B_014_1.csv\n",
      "Processed and saved samples for file: B_014_2.csv\n",
      "Processed and saved samples for file: B_014_3.csv\n",
      "Processed and saved samples for file: B_021_0.csv\n",
      "Processed and saved samples for file: B_021_1.csv\n",
      "Processed and saved samples for file: B_021_2.csv\n",
      "Processed and saved samples for file: B_021_3.csv\n",
      "Processed and saved samples for file: IR_007_0.csv\n",
      "Processed and saved samples for file: IR_007_1.csv\n",
      "Processed and saved samples for file: IR_007_2.csv\n",
      "Processed and saved samples for file: IR_007_3.csv\n",
      "Processed and saved samples for file: IR_014_0.csv\n",
      "Processed and saved samples for file: IR_014_1.csv\n",
      "Processed and saved samples for file: IR_014_2.csv\n",
      "Processed and saved samples for file: IR_014_3.csv\n",
      "Processed and saved samples for file: IR_021_0.csv\n",
      "Processed and saved samples for file: IR_021_1.csv\n",
      "Processed and saved samples for file: IR_021_2.csv\n",
      "Processed and saved samples for file: IR_021_3.csv\n",
      "Processed and saved samples for file: OR@06_007_0.csv\n",
      "Processed and saved samples for file: OR@06_007_1.csv\n",
      "Processed and saved samples for file: OR@06_007_2.csv\n",
      "Processed and saved samples for file: OR@06_007_3.csv\n",
      "Processed and saved samples for file: OR@06_014_0.csv\n",
      "Processed and saved samples for file: OR@06_014_1.csv\n",
      "Processed and saved samples for file: OR@06_014_2.csv\n",
      "Processed and saved samples for file: OR@06_014_3.csv\n",
      "Processed and saved samples for file: OR@06_021_0.csv\n",
      "Processed and saved samples for file: OR@06_021_1.csv\n",
      "Processed and saved samples for file: OR@06_021_2.csv\n",
      "Processed and saved samples for file: OR@06_021_3.csv\n"
     ]
    }
   ],
   "source": [
    "def Data_Division():\n",
    "    current_dir = os.getcwd()\n",
    "    preprocessing_path = os.path.join(current_dir, \"..\", \"..\", \"..\", \"Dataset\", \"CWRU\", \"Data Preprocessing(DE)\")\n",
    "    division_path = os.path.join(current_dir, \"..\", \"..\", \"..\", \"Dataset\", \"CWRU\", \"Data Division\")\n",
    "\n",
    "    fault_types = ['N', 'B', 'IR', 'OR@06']\n",
    "\n",
    "    # Create directories for Data Division\n",
    "    if not os.path.exists(division_path):\n",
    "        os.makedirs(division_path)\n",
    "    for fault in fault_types:\n",
    "        fault_total_path = os.path.join(division_path, fault, \"Total\")\n",
    "        fault_sample_path = os.path.join(division_path, fault, \"Samples\")\n",
    "        os.makedirs(fault_total_path, exist_ok=True)\n",
    "        os.makedirs(fault_sample_path, exist_ok=True)\n",
    "\n",
    "    # Process each file in the preprocessing directory\n",
    "    for fault in fault_types:\n",
    "        fault_dir = os.path.join(preprocessing_path, fault)\n",
    "        if not os.path.exists(fault_dir):\n",
    "            print(f\"Fault directory does not exist: {fault_dir}\")\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(fault_dir):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(fault_dir, file)\n",
    "                data = pd.read_csv(file_path, header=None).values.flatten()\n",
    "\n",
    "                # Split data into 6:2:2 ratio\n",
    "                train_data, temp_data = train_test_split(data, test_size=0.4, random_state=42)\n",
    "                val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "                fault_total_path = os.path.join(division_path, fault, \"Total\")\n",
    "                fault_sample_path = os.path.join(division_path, fault, \"Samples\")\n",
    "\n",
    "                # Save Total datasets as CSV without header\n",
    "                pd.DataFrame(train_data).to_csv(\n",
    "                    os.path.join(fault_total_path, f\"Total_Train_{file.replace('_DE', '')[:-4]}.csv\"), index=False, header=False)\n",
    "                pd.DataFrame(val_data).to_csv(\n",
    "                    os.path.join(fault_total_path, f\"Total_Validation_{file.replace('_DE', '')[:-4]}.csv\"), index=False, header=False)\n",
    "                pd.DataFrame(test_data).to_csv(\n",
    "                    os.path.join(fault_total_path, f\"Total_Test_{file.replace('_DE', '')[:-4]}.csv\"), index=False, header=False)\n",
    "\n",
    "                # Create samples for train, validation, and test\n",
    "                # 여기서 마지막 남은 샘플은 안만들게 된다!\n",
    "                def create_samples(dataset, dataset_name):\n",
    "                    sample_size = 4096\n",
    "                    shift_size = 2048\n",
    "                    num_samples = (len(dataset) // shift_size) - 1\n",
    "\n",
    "                    for i in range(num_samples):\n",
    "                        start_idx = i * shift_size\n",
    "                        end_idx = start_idx + sample_size\n",
    "                        sample = dataset[start_idx:end_idx]\n",
    "\n",
    "                        sample_file_name = f\"{dataset_name}_{i+1}.csv\"\n",
    "                        pd.DataFrame(sample).to_csv(\n",
    "                            os.path.join(fault_sample_path, sample_file_name), index=False, header=False)\n",
    "\n",
    "                # Generate samples\n",
    "                create_samples(train_data, f\"Train_{file.replace('_DE', '')[:-4]}\")\n",
    "                create_samples(val_data, f\"Validation_{file.replace('_DE', '')[:-4]}\")\n",
    "                create_samples(test_data, f\"Test_{file.replace('_DE', '')[:-4]}\")\n",
    "\n",
    "                print(f\"Processed and saved samples for file: {file}\")\n",
    "\n",
    "Data_Division()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Type 별 Train, Validation, Test 샘플 갯수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def Count_Sample_Files(Data):\n",
    "    if Data == \"CWRU\":\n",
    "        current_dir = os.getcwd()\n",
    "        division_path = os.path.join(current_dir, \"..\", \"..\", \"..\", \"Dataset\", \"CWRU\", \"Data Division\")\n",
    "\n",
    "        fault_types = ['N', 'B', 'IR', 'OR@06']\n",
    "        counts = []\n",
    "\n",
    "        for fault in fault_types:\n",
    "            fault_sample_path = os.path.join(division_path, fault, \"Samples\")\n",
    "            if not os.path.exists(fault_sample_path):\n",
    "                print(f\"Samples directory does not exist for Fault Type {fault}: {fault_sample_path}\")\n",
    "                continue\n",
    "\n",
    "            fault_counts = {\n",
    "                \"Fault_Type\": fault,\n",
    "                \"Train\": 0,\n",
    "                \"Validation\": 0,\n",
    "                \"Test\": 0,\n",
    "                \"Train_007\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Validation_007\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Test_007\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Train_014\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Validation_014\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Test_014\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Train_021\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Validation_021\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0},\n",
    "                \"Test_021\": {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0}\n",
    "            }\n",
    "\n",
    "            # Count files in Samples directory\n",
    "            for file in os.listdir(fault_sample_path):\n",
    "                parts = file.split('_')\n",
    "                if len(parts) < 4:\n",
    "                    continue\n",
    "\n",
    "                dataset_type, fault_type, rpm, index = parts[0], parts[1], parts[2], parts[3]\n",
    "                if dataset_type in [\"Train\", \"Validation\", \"Test\"] and rpm in [\"007\", \"014\", \"021\"] and index in [\"0\", \"1\", \"2\", \"3\"]:\n",
    "                    key = f\"{dataset_type}_{rpm}\"\n",
    "                    fault_counts[key][index] += 1\n",
    "                    fault_counts[dataset_type] += 1\n",
    "\n",
    "            counts.append(fault_counts)\n",
    "\n",
    "        # Flatten nested dictionaries for DataFrame\n",
    "        flattened_counts = []\n",
    "        for count in counts:\n",
    "            flat_count = {\"Fault_Type\": count[\"Fault_Type\"], \"Train\": count[\"Train\"], \"Validation\": count[\"Validation\"], \"Test\": count[\"Test\"]}\n",
    "            for key, subdict in count.items():\n",
    "                if isinstance(subdict, dict):\n",
    "                    for subkey, value in subdict.items():\n",
    "                        flat_count[f\"{key}_{subkey}\"] = value\n",
    "            flattened_counts.append(flat_count)\n",
    "\n",
    "        # Convert counts to DataFrame and display\n",
    "        df_counts = pd.DataFrame(flattened_counts)\n",
    "        return df_counts\n",
    "\n",
    "    else:\n",
    "        print(\"This function is currently implemented for the CWRU dataset only.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fault_Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "      <th>Train_007_0</th>\n",
       "      <th>Train_007_1</th>\n",
       "      <th>Train_007_2</th>\n",
       "      <th>Train_007_3</th>\n",
       "      <th>Validation_007_0</th>\n",
       "      <th>Validation_007_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Train_021_2</th>\n",
       "      <th>Train_021_3</th>\n",
       "      <th>Validation_021_0</th>\n",
       "      <th>Validation_021_1</th>\n",
       "      <th>Validation_021_2</th>\n",
       "      <th>Validation_021_3</th>\n",
       "      <th>Test_021_0</th>\n",
       "      <th>Test_021_1</th>\n",
       "      <th>Test_021_2</th>\n",
       "      <th>Test_021_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>408</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IR</td>\n",
       "      <td>409</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OR@06</td>\n",
       "      <td>408</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Fault_Type  Train  Validation  Test  Train_007_0  Train_007_1  Train_007_2  \\\n",
       "0          N      0           0     0            0            0            0   \n",
       "1          B    408         120   120           34           34           34   \n",
       "2         IR    409         121   121           34           34           34   \n",
       "3      OR@06    408         120   120           34           34           34   \n",
       "\n",
       "   Train_007_3  Validation_007_0  Validation_007_1  ...  Train_021_2  \\\n",
       "0            0                 0                 0  ...            0   \n",
       "1           34                10                10  ...           34   \n",
       "2           35                10                10  ...           34   \n",
       "3           34                10                10  ...           34   \n",
       "\n",
       "   Train_021_3  Validation_021_0  Validation_021_1  Validation_021_2  \\\n",
       "0            0                 0                 0                 0   \n",
       "1           34                10                10                10   \n",
       "2           34                10                10                10   \n",
       "3           34                10                10                10   \n",
       "\n",
       "   Validation_021_3  Test_021_0  Test_021_1  Test_021_2  Test_021_3  \n",
       "0                 0           0           0           0           0  \n",
       "1                10          10          10          10          10  \n",
       "2                10          10          10          10          10  \n",
       "3                10          10          10          10          10  \n",
       "\n",
       "[4 rows x 40 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Count_Sample_Files('CWRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count_Samples_Files 함수 정의\n",
    "\n",
    "def Count_Sample_Files():\n",
    "    current_dir = os.getcwd()\n",
    "    division_path = os.path.join(current_dir, \"..\", \"..\", \"..\", \"Dataset\", \"CWRU\", \"Data Division\")\n",
    "    \n",
    "    fault_types = ['N', 'B', 'IR', 'OR@06']\n",
    "    counts = []\n",
    "\n",
    "    for fault in fault_types:\n",
    "        fault_sample_path = os.path.join(division_path, fault, \"Samples\")\n",
    "        if not os.path.exists(fault_sample_path):\n",
    "            print(f\"Samples directory does not exist for Fault Type {fault}: {fault_sample_path}\")\n",
    "            continue\n",
    "\n",
    "        fault_counts = {\n",
    "            \"Fault_Type\": fault,\n",
    "            \"Train\": 0,\n",
    "            \"Validation\": 0,\n",
    "            \"Test\": 0,\n",
    "            \"Train_007\": 0,\n",
    "            \"Validation_007\": 0,\n",
    "            \"Test_007\": 0,\n",
    "            \"Train_014\": 0,\n",
    "            \"Validation_014\": 0,\n",
    "            \"Test_014\": 0,\n",
    "            \"Train_021\": 0,\n",
    "            \"Validation_021\": 0,\n",
    "            \"Test_021\": 0\n",
    "        }\n",
    "\n",
    "        # Count files in Samples directory\n",
    "        for file in os.listdir(fault_sample_path):\n",
    "            if file.startswith(\"Train\"):\n",
    "                fault_counts[\"Train\"] += 1\n",
    "                if \"007\" in file:\n",
    "                    fault_counts[\"Train_007\"] += 1\n",
    "                elif \"014\" in file:\n",
    "                    fault_counts[\"Train_014\"] += 1\n",
    "                elif \"021\" in file:\n",
    "                    fault_counts[\"Train_021\"] += 1\n",
    "            elif file.startswith(\"Validation\"):\n",
    "                fault_counts[\"Validation\"] += 1\n",
    "                if \"007\" in file:\n",
    "                    fault_counts[\"Validation_007\"] += 1\n",
    "                elif \"014\" in file:\n",
    "                    fault_counts[\"Validation_014\"] += 1\n",
    "                elif \"021\" in file:\n",
    "                    fault_counts[\"Validation_021\"] += 1\n",
    "            elif file.startswith(\"Test\"):\n",
    "                fault_counts[\"Test\"] += 1\n",
    "                if \"007\" in file:\n",
    "                    fault_counts[\"Test_007\"] += 1\n",
    "                elif \"014\" in file:\n",
    "                    fault_counts[\"Test_014\"] += 1\n",
    "                elif \"021\" in file:\n",
    "                    fault_counts[\"Test_021\"] += 1\n",
    "\n",
    "        counts.append(fault_counts)\n",
    "\n",
    "    # Convert counts to DataFrame and display\n",
    "    df_counts = pd.DataFrame(counts)\n",
    "    return df_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fault_Type</th>\n",
       "      <th>Train</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Test</th>\n",
       "      <th>Train_007</th>\n",
       "      <th>Validation_007</th>\n",
       "      <th>Test_007</th>\n",
       "      <th>Train_014</th>\n",
       "      <th>Validation_014</th>\n",
       "      <th>Test_014</th>\n",
       "      <th>Train_021</th>\n",
       "      <th>Validation_021</th>\n",
       "      <th>Test_021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N</td>\n",
       "      <td>491</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>408</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IR</td>\n",
       "      <td>409</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>137</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OR@06</td>\n",
       "      <td>408</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>136</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Fault_Type  Train  Validation  Test  Train_007  Validation_007  Test_007  \\\n",
       "0          N    491         160   160          0               0         0   \n",
       "1          B    408         120   120        136              40        40   \n",
       "2         IR    409         121   121        137              41        41   \n",
       "3      OR@06    408         120   120        136              40        40   \n",
       "\n",
       "   Train_014  Validation_014  Test_014  Train_021  Validation_021  Test_021  \n",
       "0          0               0         0          0               0         0  \n",
       "1        136              40        40        136              40        40  \n",
       "2        136              40        40        136              40        40  \n",
       "3        136              40        40        136              40        40  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Count_Sample_Files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 007, 014, 021에 상관없이  \n",
    "- 또, HP가 0,1,2,3인지 아닌지에 상관없이 Fault Type을 싹 묶어서 Train, Val, Test로 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2. 클래스별 라벨링\n",
    "N : 0, IR : 1, OR : 2, B : 3  \n",
    "'{Train or Validation or Test}_{Fault Type}_{007 or 014 or 021}_{0 or 1 or 2 or 3}_{Sample Number}'  \n",
    "따라서 Fault Type에 맞게 자동 라벨링을 진행시키자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3. WDCNN Baseline Model 설계  \n",
    "\n",
    "해결해야 하는 문제\n",
    "1. Validation과 Test의 Shuffle\n",
    "2. WDCNN 구조 파악 -> 과연 잘 적용 되었는지?\n",
    "3. 하이퍼파라미터 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChoiSeongHyeon\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000000: LR: 0.100000, Train Loss: 41.7648, Train Acc: 0.3520, Val Loss: 17.4502, Val Acc: 0.4798\n",
      "✅ Best model saved at epoch 1 with val_acc 0.4798\n",
      "Epoch 2/1000000: LR: 0.100000, Train Loss: 15.8251, Train Acc: 0.4580, Val Loss: 7.6988, Val Acc: 0.5432\n",
      "✅ Best model saved at epoch 2 with val_acc 0.5432\n",
      "Epoch 3/1000000: LR: 0.100000, Train Loss: 14.7122, Train Acc: 0.5210, Val Loss: 5.9729, Val Acc: 0.5585\n",
      "✅ Best model saved at epoch 3 with val_acc 0.5585\n",
      "Epoch 4/1000000: LR: 0.100000, Train Loss: 12.2808, Train Acc: 0.6037, Val Loss: 4.7762, Val Acc: 0.6833\n",
      "✅ Best model saved at epoch 4 with val_acc 0.6833\n",
      "Epoch 5/1000000: LR: 0.100000, Train Loss: 9.2673, Train Acc: 0.7314, Val Loss: 4.5635, Val Acc: 0.7735\n",
      "✅ Best model saved at epoch 5 with val_acc 0.7735\n",
      "Epoch 6/1000000: LR: 0.100000, Train Loss: 8.5500, Train Acc: 0.7669, Val Loss: 5.1368, Val Acc: 0.6948\n",
      "Epoch 7/1000000: LR: 0.100000, Train Loss: 6.7976, Train Acc: 0.8071, Val Loss: 1.6660, Val Acc: 0.8925\n",
      "✅ Best model saved at epoch 7 with val_acc 0.8925\n",
      "Epoch 8/1000000: LR: 0.100000, Train Loss: 5.8143, Train Acc: 0.8456, Val Loss: 5.0838, Val Acc: 0.8311\n",
      "Epoch 9/1000000: LR: 0.100000, Train Loss: 4.9085, Train Acc: 0.8636, Val Loss: 2.9054, Val Acc: 0.8464\n",
      "Epoch 10/1000000: LR: 0.100000, Train Loss: 4.3467, Train Acc: 0.8730, Val Loss: 0.8775, Val Acc: 0.9578\n",
      "✅ Best model saved at epoch 10 with val_acc 0.9578\n",
      "Epoch 11/1000000: LR: 0.100000, Train Loss: 3.6047, Train Acc: 0.8992, Val Loss: 1.0775, Val Acc: 0.9155\n",
      "Epoch 12/1000000: LR: 0.100000, Train Loss: 3.9612, Train Acc: 0.8928, Val Loss: 0.8959, Val Acc: 0.9040\n",
      "Epoch 13/1000000: LR: 0.100000, Train Loss: 7.2381, Train Acc: 0.8118, Val Loss: 2.8315, Val Acc: 0.8100\n",
      "Epoch 14/1000000: LR: 0.100000, Train Loss: 6.3242, Train Acc: 0.8042, Val Loss: 3.0462, Val Acc: 0.8407\n",
      "Epoch 15/1000000: LR: 0.100000, Train Loss: 3.7226, Train Acc: 0.8992, Val Loss: 1.8159, Val Acc: 0.8714\n",
      "Epoch 16/1000000: LR: 0.100000, Train Loss: 3.3157, Train Acc: 0.9161, Val Loss: 0.9989, Val Acc: 0.9347\n",
      "Epoch 17/1000000: LR: 0.100000, Train Loss: 3.9674, Train Acc: 0.8811, Val Loss: 1.1380, Val Acc: 0.8791\n",
      "Epoch 18/1000000: LR: 0.100000, Train Loss: 3.2481, Train Acc: 0.9103, Val Loss: 1.0286, Val Acc: 0.9060\n",
      "Epoch 19/1000000: LR: 0.100000, Train Loss: 2.7736, Train Acc: 0.9178, Val Loss: 1.2285, Val Acc: 0.9079\n",
      "Epoch 20/1000000: LR: 0.100000, Train Loss: 2.2094, Train Acc: 0.9411, Val Loss: 1.4765, Val Acc: 0.9309\n",
      "Epoch 21/1000000: LR: 0.100000, Train Loss: 2.9143, Train Acc: 0.9108, Val Loss: 1.0157, Val Acc: 0.9251\n",
      "Epoch 22/1000000: LR: 0.100000, Train Loss: 3.0142, Train Acc: 0.9196, Val Loss: 2.0259, Val Acc: 0.8426\n",
      "Epoch 23/1000000: LR: 0.100000, Train Loss: 2.2959, Train Acc: 0.9394, Val Loss: 1.5651, Val Acc: 0.9060\n",
      "Epoch 24/1000000: LR: 0.100000, Train Loss: 3.6113, Train Acc: 0.9056, Val Loss: 2.1310, Val Acc: 0.8580\n",
      "Epoch 25/1000000: LR: 0.100000, Train Loss: 5.8279, Train Acc: 0.8467, Val Loss: 2.0514, Val Acc: 0.8157\n",
      "Epoch 26/1000000: LR: 0.100000, Train Loss: 3.8172, Train Acc: 0.8992, Val Loss: 4.2336, Val Acc: 0.8004\n",
      "Epoch 27/1000000: LR: 0.100000, Train Loss: 4.9565, Train Acc: 0.8747, Val Loss: 4.8834, Val Acc: 0.8311\n",
      "Epoch 28/1000000: LR: 0.100000, Train Loss: 3.4109, Train Acc: 0.9277, Val Loss: 2.2152, Val Acc: 0.9040\n",
      "Epoch 29/1000000: LR: 0.100000, Train Loss: 2.7084, Train Acc: 0.9260, Val Loss: 2.4091, Val Acc: 0.8407\n",
      "Epoch 30/1000000: LR: 0.100000, Train Loss: 2.3267, Train Acc: 0.9406, Val Loss: 1.8464, Val Acc: 0.8349\n",
      "Epoch 31/1000000: LR: 0.010000, Train Loss: 3.4772, Train Acc: 0.9097, Val Loss: 1.6515, Val Acc: 0.8983\n",
      "Epoch 32/1000000: LR: 0.010000, Train Loss: 2.8049, Train Acc: 0.9254, Val Loss: 1.1543, Val Acc: 0.9213\n",
      "Epoch 33/1000000: LR: 0.010000, Train Loss: 2.2163, Train Acc: 0.9376, Val Loss: 1.1417, Val Acc: 0.9232\n",
      "Epoch 34/1000000: LR: 0.010000, Train Loss: 1.7069, Train Acc: 0.9580, Val Loss: 1.2206, Val Acc: 0.9251\n",
      "Epoch 35/1000000: LR: 0.010000, Train Loss: 1.5389, Train Acc: 0.9592, Val Loss: 1.1228, Val Acc: 0.9290\n",
      "Epoch 36/1000000: LR: 0.010000, Train Loss: 1.4800, Train Acc: 0.9569, Val Loss: 1.0998, Val Acc: 0.9347\n",
      "Epoch 37/1000000: LR: 0.010000, Train Loss: 2.3225, Train Acc: 0.9330, Val Loss: 1.1195, Val Acc: 0.9309\n",
      "Epoch 38/1000000: LR: 0.010000, Train Loss: 1.6712, Train Acc: 0.9476, Val Loss: 1.0989, Val Acc: 0.9309\n",
      "Epoch 39/1000000: LR: 0.010000, Train Loss: 1.4091, Train Acc: 0.9714, Val Loss: 1.1822, Val Acc: 0.9271\n",
      "Epoch 40/1000000: LR: 0.010000, Train Loss: 1.2156, Train Acc: 0.9837, Val Loss: 1.1935, Val Acc: 0.9309\n",
      "Epoch 41/1000000: LR: 0.010000, Train Loss: 1.2770, Train Acc: 0.9825, Val Loss: 1.1873, Val Acc: 0.9309\n",
      "Epoch 42/1000000: LR: 0.010000, Train Loss: 1.2607, Train Acc: 0.9814, Val Loss: 1.2387, Val Acc: 0.9309\n",
      "Epoch 43/1000000: LR: 0.010000, Train Loss: 1.8319, Train Acc: 0.9592, Val Loss: 1.2453, Val Acc: 0.9251\n",
      "Epoch 44/1000000: LR: 0.010000, Train Loss: 1.4018, Train Acc: 0.9679, Val Loss: 1.1737, Val Acc: 0.9309\n",
      "Epoch 45/1000000: LR: 0.010000, Train Loss: 1.2037, Train Acc: 0.9796, Val Loss: 1.2379, Val Acc: 0.9309\n",
      "Epoch 46/1000000: LR: 0.010000, Train Loss: 1.1089, Train Acc: 0.9848, Val Loss: 1.2352, Val Acc: 0.9271\n",
      "Epoch 47/1000000: LR: 0.010000, Train Loss: 0.9568, Train Acc: 0.9848, Val Loss: 1.2224, Val Acc: 0.9271\n",
      "Epoch 48/1000000: LR: 0.010000, Train Loss: 2.1439, Train Acc: 0.9580, Val Loss: 1.3464, Val Acc: 0.9251\n",
      "Epoch 49/1000000: LR: 0.010000, Train Loss: 2.2667, Train Acc: 0.9464, Val Loss: 1.3078, Val Acc: 0.9232\n",
      "Epoch 50/1000000: LR: 0.010000, Train Loss: 1.4734, Train Acc: 0.9674, Val Loss: 1.2988, Val Acc: 0.9175\n",
      "Epoch 51/1000000: LR: 0.010000, Train Loss: 1.2328, Train Acc: 0.9755, Val Loss: 1.1748, Val Acc: 0.9271\n",
      "Epoch 52/1000000: LR: 0.001000, Train Loss: 2.9705, Train Acc: 0.9330, Val Loss: 1.1718, Val Acc: 0.9309\n",
      "Epoch 53/1000000: LR: 0.001000, Train Loss: 0.9820, Train Acc: 0.9837, Val Loss: 1.1974, Val Acc: 0.9309\n",
      "Epoch 54/1000000: LR: 0.001000, Train Loss: 1.0471, Train Acc: 0.9819, Val Loss: 1.2724, Val Acc: 0.9271\n",
      "Epoch 55/1000000: LR: 0.001000, Train Loss: 1.4011, Train Acc: 0.9738, Val Loss: 1.2582, Val Acc: 0.9290\n",
      "Epoch 56/1000000: LR: 0.001000, Train Loss: 1.6713, Train Acc: 0.9551, Val Loss: 1.2933, Val Acc: 0.9309\n",
      "Epoch 57/1000000: LR: 0.001000, Train Loss: 1.1100, Train Acc: 0.9831, Val Loss: 1.2883, Val Acc: 0.9290\n",
      "Epoch 58/1000000: LR: 0.001000, Train Loss: 1.2298, Train Acc: 0.9744, Val Loss: 1.2698, Val Acc: 0.9290\n",
      "Epoch 59/1000000: LR: 0.001000, Train Loss: 1.3071, Train Acc: 0.9796, Val Loss: 1.2724, Val Acc: 0.9271\n",
      "Epoch 60/1000000: LR: 0.001000, Train Loss: 1.0230, Train Acc: 0.9854, Val Loss: 1.2891, Val Acc: 0.9271\n",
      "Epoch 61/1000000: LR: 0.001000, Train Loss: 0.9096, Train Acc: 0.9854, Val Loss: 1.3341, Val Acc: 0.9290\n",
      "Epoch 62/1000000: LR: 0.001000, Train Loss: 1.4763, Train Acc: 0.9819, Val Loss: 1.2721, Val Acc: 0.9290\n",
      "Epoch 63/1000000: LR: 0.001000, Train Loss: 1.2044, Train Acc: 0.9767, Val Loss: 1.3148, Val Acc: 0.9290\n",
      "Epoch 64/1000000: LR: 0.001000, Train Loss: 1.2930, Train Acc: 0.9732, Val Loss: 1.1745, Val Acc: 0.9271\n",
      "Epoch 65/1000000: LR: 0.001000, Train Loss: 1.4160, Train Acc: 0.9685, Val Loss: 1.2000, Val Acc: 0.9251\n",
      "Epoch 66/1000000: LR: 0.001000, Train Loss: 1.6551, Train Acc: 0.9633, Val Loss: 1.2312, Val Acc: 0.9328\n",
      "Epoch 67/1000000: LR: 0.001000, Train Loss: 1.6515, Train Acc: 0.9645, Val Loss: 1.2688, Val Acc: 0.9290\n",
      "Epoch 68/1000000: LR: 0.001000, Train Loss: 1.0516, Train Acc: 0.9831, Val Loss: 1.3054, Val Acc: 0.9251\n",
      "Epoch 69/1000000: LR: 0.001000, Train Loss: 0.8278, Train Acc: 0.9907, Val Loss: 1.2817, Val Acc: 0.9290\n",
      "Epoch 70/1000000: LR: 0.001000, Train Loss: 1.0070, Train Acc: 0.9825, Val Loss: 1.2532, Val Acc: 0.9271\n",
      "Epoch 71/1000000: LR: 0.001000, Train Loss: 1.1424, Train Acc: 0.9825, Val Loss: 1.2110, Val Acc: 0.9251\n",
      "Epoch 72/1000000: LR: 0.001000, Train Loss: 1.0881, Train Acc: 0.9802, Val Loss: 1.2502, Val Acc: 0.9290\n",
      "Epoch 73/1000000: LR: 0.000100, Train Loss: 0.9366, Train Acc: 0.9831, Val Loss: 1.2452, Val Acc: 0.9232\n",
      "Epoch 74/1000000: LR: 0.000100, Train Loss: 0.9085, Train Acc: 0.9860, Val Loss: 1.2954, Val Acc: 0.9290\n",
      "Epoch 75/1000000: LR: 0.000100, Train Loss: 1.0015, Train Acc: 0.9843, Val Loss: 1.2104, Val Acc: 0.9251\n",
      "Epoch 76/1000000: LR: 0.000100, Train Loss: 1.1359, Train Acc: 0.9726, Val Loss: 1.2294, Val Acc: 0.9328\n",
      "Epoch 77/1000000: LR: 0.000100, Train Loss: 1.2870, Train Acc: 0.9773, Val Loss: 1.2570, Val Acc: 0.9251\n",
      "Epoch 78/1000000: LR: 0.000100, Train Loss: 1.0467, Train Acc: 0.9825, Val Loss: 1.2326, Val Acc: 0.9232\n",
      "Epoch 79/1000000: LR: 0.000100, Train Loss: 0.9502, Train Acc: 0.9866, Val Loss: 1.2162, Val Acc: 0.9251\n",
      "Epoch 80/1000000: LR: 0.000100, Train Loss: 1.0022, Train Acc: 0.9872, Val Loss: 1.2453, Val Acc: 0.9251\n",
      "Epoch 81/1000000: LR: 0.000100, Train Loss: 0.9154, Train Acc: 0.9883, Val Loss: 1.2242, Val Acc: 0.9251\n",
      "Epoch 82/1000000: LR: 0.000100, Train Loss: 1.0191, Train Acc: 0.9814, Val Loss: 1.2181, Val Acc: 0.9347\n",
      "Epoch 83/1000000: LR: 0.000100, Train Loss: 0.9295, Train Acc: 0.9883, Val Loss: 1.2403, Val Acc: 0.9232\n",
      "Epoch 84/1000000: LR: 0.000100, Train Loss: 1.0419, Train Acc: 0.9854, Val Loss: 1.2539, Val Acc: 0.9271\n",
      "Epoch 85/1000000: LR: 0.000100, Train Loss: 1.5123, Train Acc: 0.9726, Val Loss: 1.2283, Val Acc: 0.9251\n",
      "Epoch 86/1000000: LR: 0.000100, Train Loss: 1.0493, Train Acc: 0.9825, Val Loss: 1.2069, Val Acc: 0.9328\n",
      "Epoch 87/1000000: LR: 0.000100, Train Loss: 1.1771, Train Acc: 0.9808, Val Loss: 1.2223, Val Acc: 0.9271\n",
      "Epoch 88/1000000: LR: 0.000100, Train Loss: 0.9213, Train Acc: 0.9848, Val Loss: 1.2744, Val Acc: 0.9251\n",
      "Epoch 89/1000000: LR: 0.000100, Train Loss: 1.3602, Train Acc: 0.9732, Val Loss: 1.2993, Val Acc: 0.9290\n",
      "Epoch 90/1000000: LR: 0.000100, Train Loss: 1.5923, Train Acc: 0.9755, Val Loss: 1.2806, Val Acc: 0.9251\n",
      "Epoch 91/1000000: LR: 0.000100, Train Loss: 1.6233, Train Acc: 0.9703, Val Loss: 1.2184, Val Acc: 0.9251\n",
      "Epoch 92/1000000: LR: 0.000100, Train Loss: 1.2588, Train Acc: 0.9767, Val Loss: 1.2525, Val Acc: 0.9290\n",
      "Epoch 93/1000000: LR: 0.000100, Train Loss: 1.1825, Train Acc: 0.9732, Val Loss: 1.2217, Val Acc: 0.9271\n",
      "Epoch 94/1000000: LR: 0.000010, Train Loss: 1.0463, Train Acc: 0.9878, Val Loss: 1.2231, Val Acc: 0.9271\n",
      "Epoch 95/1000000: LR: 0.000010, Train Loss: 1.1757, Train Acc: 0.9761, Val Loss: 1.3039, Val Acc: 0.9271\n",
      "Epoch 96/1000000: LR: 0.000010, Train Loss: 1.2942, Train Acc: 0.9738, Val Loss: 1.2215, Val Acc: 0.9271\n",
      "Epoch 97/1000000: LR: 0.000010, Train Loss: 1.0278, Train Acc: 0.9825, Val Loss: 1.2442, Val Acc: 0.9367\n",
      "Epoch 98/1000000: LR: 0.000010, Train Loss: 1.4629, Train Acc: 0.9627, Val Loss: 1.2505, Val Acc: 0.9309\n",
      "Epoch 99/1000000: LR: 0.000010, Train Loss: 0.7987, Train Acc: 0.9901, Val Loss: 1.3239, Val Acc: 0.9251\n",
      "Epoch 100/1000000: LR: 0.000010, Train Loss: 1.7532, Train Acc: 0.9674, Val Loss: 1.2685, Val Acc: 0.9290\n",
      "Epoch 101/1000000: LR: 0.000010, Train Loss: 0.9766, Train Acc: 0.9825, Val Loss: 1.2656, Val Acc: 0.9271\n",
      "Epoch 102/1000000: LR: 0.000010, Train Loss: 1.1963, Train Acc: 0.9767, Val Loss: 1.2393, Val Acc: 0.9232\n",
      "Epoch 103/1000000: LR: 0.000010, Train Loss: 1.2269, Train Acc: 0.9779, Val Loss: 1.2569, Val Acc: 0.9251\n",
      "Epoch 104/1000000: LR: 0.000010, Train Loss: 1.2384, Train Acc: 0.9749, Val Loss: 1.3222, Val Acc: 0.9271\n",
      "Epoch 105/1000000: LR: 0.000010, Train Loss: 1.1341, Train Acc: 0.9784, Val Loss: 1.2614, Val Acc: 0.9251\n",
      "Epoch 106/1000000: LR: 0.000010, Train Loss: 0.9446, Train Acc: 0.9878, Val Loss: 1.2548, Val Acc: 0.9271\n",
      "Epoch 107/1000000: LR: 0.000010, Train Loss: 1.0680, Train Acc: 0.9843, Val Loss: 1.2357, Val Acc: 0.9309\n",
      "Epoch 108/1000000: LR: 0.000010, Train Loss: 1.0369, Train Acc: 0.9814, Val Loss: 1.2018, Val Acc: 0.9271\n",
      "Epoch 109/1000000: LR: 0.000010, Train Loss: 0.9958, Train Acc: 0.9848, Val Loss: 1.2356, Val Acc: 0.9271\n",
      "Epoch 110/1000000: LR: 0.000010, Train Loss: 0.9968, Train Acc: 0.9843, Val Loss: 1.2628, Val Acc: 0.9251\n",
      "Epoch 111/1000000: LR: 0.000010, Train Loss: 1.3058, Train Acc: 0.9808, Val Loss: 1.2661, Val Acc: 0.9271\n",
      "Epoch 112/1000000: LR: 0.000010, Train Loss: 1.9378, Train Acc: 0.9604, Val Loss: 1.2130, Val Acc: 0.9251\n",
      "Epoch 113/1000000: LR: 0.000010, Train Loss: 0.8692, Train Acc: 0.9837, Val Loss: 1.2212, Val Acc: 0.9271\n",
      "Epoch 114/1000000: LR: 0.000010, Train Loss: 1.2182, Train Acc: 0.9761, Val Loss: 1.2308, Val Acc: 0.9271\n",
      "Epoch 115/1000000: LR: 0.000001, Train Loss: 1.1178, Train Acc: 0.9837, Val Loss: 1.3008, Val Acc: 0.9271\n",
      "Epoch 116/1000000: LR: 0.000001, Train Loss: 1.1758, Train Acc: 0.9814, Val Loss: 1.2770, Val Acc: 0.9271\n",
      "Epoch 117/1000000: LR: 0.000001, Train Loss: 2.6781, Train Acc: 0.9796, Val Loss: 1.3798, Val Acc: 0.9136\n",
      "Epoch 118/1000000: LR: 0.000001, Train Loss: 1.7554, Train Acc: 0.9633, Val Loss: 1.2307, Val Acc: 0.9232\n",
      "Epoch 119/1000000: LR: 0.000001, Train Loss: 1.9470, Train Acc: 0.9493, Val Loss: 1.2477, Val Acc: 0.9232\n",
      "Epoch 120/1000000: LR: 0.000001, Train Loss: 1.3199, Train Acc: 0.9790, Val Loss: 1.2831, Val Acc: 0.9271\n",
      "Epoch 121/1000000: LR: 0.000001, Train Loss: 1.2398, Train Acc: 0.9767, Val Loss: 1.2602, Val Acc: 0.9251\n",
      "Epoch 122/1000000: LR: 0.000001, Train Loss: 1.2549, Train Acc: 0.9790, Val Loss: 1.2205, Val Acc: 0.9386\n",
      "Epoch 123/1000000: LR: 0.000001, Train Loss: 1.1030, Train Acc: 0.9802, Val Loss: 1.2890, Val Acc: 0.9290\n",
      "Epoch 124/1000000: LR: 0.000001, Train Loss: 1.2683, Train Acc: 0.9755, Val Loss: 1.2187, Val Acc: 0.9251\n",
      "Epoch 125/1000000: LR: 0.000001, Train Loss: 1.6441, Train Acc: 0.9575, Val Loss: 1.2541, Val Acc: 0.9251\n",
      "Epoch 126/1000000: LR: 0.000001, Train Loss: 1.1398, Train Acc: 0.9744, Val Loss: 1.2535, Val Acc: 0.9290\n",
      "Epoch 127/1000000: LR: 0.000001, Train Loss: 1.1396, Train Acc: 0.9802, Val Loss: 1.2375, Val Acc: 0.9290\n",
      "Epoch 128/1000000: LR: 0.000001, Train Loss: 0.8284, Train Acc: 0.9913, Val Loss: 1.4194, Val Acc: 0.9290\n",
      "Epoch 129/1000000: LR: 0.000001, Train Loss: 0.9396, Train Acc: 0.9883, Val Loss: 1.2558, Val Acc: 0.9271\n",
      "Epoch 130/1000000: LR: 0.000001, Train Loss: 0.9926, Train Acc: 0.9843, Val Loss: 1.3061, Val Acc: 0.9271\n",
      "Epoch 131/1000000: LR: 0.000001, Train Loss: 1.4788, Train Acc: 0.9662, Val Loss: 1.2443, Val Acc: 0.9251\n",
      "Epoch 132/1000000: LR: 0.000001, Train Loss: 0.9735, Train Acc: 0.9866, Val Loss: 1.2924, Val Acc: 0.9271\n",
      "Epoch 133/1000000: LR: 0.000001, Train Loss: 1.1738, Train Acc: 0.9761, Val Loss: 1.2771, Val Acc: 0.9271\n",
      "Epoch 134/1000000: LR: 0.000001, Train Loss: 1.0870, Train Acc: 0.9796, Val Loss: 1.2380, Val Acc: 0.9271\n",
      "Epoch 135/1000000: LR: 0.000001, Train Loss: 1.0304, Train Acc: 0.9848, Val Loss: 1.2135, Val Acc: 0.9251\n",
      "Epoch 136/1000000: LR: 0.000000, Train Loss: 1.0918, Train Acc: 0.9814, Val Loss: 1.2016, Val Acc: 0.9328\n",
      "Epoch 137/1000000: LR: 0.000000, Train Loss: 1.0918, Train Acc: 0.9808, Val Loss: 1.2346, Val Acc: 0.9232\n",
      "Epoch 138/1000000: LR: 0.000000, Train Loss: 1.0598, Train Acc: 0.9802, Val Loss: 1.2473, Val Acc: 0.9271\n",
      "Epoch 139/1000000: LR: 0.000000, Train Loss: 1.2004, Train Acc: 0.9761, Val Loss: 1.2012, Val Acc: 0.9367\n",
      "Epoch 140/1000000: LR: 0.000000, Train Loss: 0.9951, Train Acc: 0.9843, Val Loss: 1.2179, Val Acc: 0.9328\n",
      "Epoch 141/1000000: LR: 0.000000, Train Loss: 1.0335, Train Acc: 0.9860, Val Loss: 1.2066, Val Acc: 0.9309\n",
      "Epoch 142/1000000: LR: 0.000000, Train Loss: 1.0977, Train Acc: 0.9831, Val Loss: 1.3003, Val Acc: 0.9251\n",
      "Epoch 143/1000000: LR: 0.000000, Train Loss: 1.0771, Train Acc: 0.9837, Val Loss: 1.2967, Val Acc: 0.9309\n",
      "Epoch 144/1000000: LR: 0.000000, Train Loss: 1.8408, Train Acc: 0.9522, Val Loss: 1.2692, Val Acc: 0.9251\n",
      "Epoch 145/1000000: LR: 0.000000, Train Loss: 1.1617, Train Acc: 0.9819, Val Loss: 1.3104, Val Acc: 0.9271\n",
      "Epoch 146/1000000: LR: 0.000000, Train Loss: 0.9497, Train Acc: 0.9831, Val Loss: 1.2941, Val Acc: 0.9309\n",
      "Epoch 147/1000000: LR: 0.000000, Train Loss: 0.8991, Train Acc: 0.9878, Val Loss: 1.2598, Val Acc: 0.9251\n",
      "Epoch 148/1000000: LR: 0.000000, Train Loss: 1.4580, Train Acc: 0.9645, Val Loss: 1.2287, Val Acc: 0.9271\n",
      "Epoch 149/1000000: LR: 0.000000, Train Loss: 1.2388, Train Acc: 0.9749, Val Loss: 1.2289, Val Acc: 0.9271\n",
      "Epoch 150/1000000: LR: 0.000000, Train Loss: 1.3780, Train Acc: 0.9662, Val Loss: 1.2032, Val Acc: 0.9290\n",
      "Epoch 151/1000000: LR: 0.000000, Train Loss: 1.5535, Train Acc: 0.9645, Val Loss: 1.2136, Val Acc: 0.9405\n",
      "Epoch 152/1000000: LR: 0.000000, Train Loss: 1.1166, Train Acc: 0.9819, Val Loss: 1.2042, Val Acc: 0.9251\n",
      "Epoch 153/1000000: LR: 0.000000, Train Loss: 1.2259, Train Acc: 0.9714, Val Loss: 1.2347, Val Acc: 0.9386\n",
      "Epoch 154/1000000: LR: 0.000000, Train Loss: 0.9993, Train Acc: 0.9837, Val Loss: 1.2500, Val Acc: 0.9271\n",
      "Epoch 155/1000000: LR: 0.000000, Train Loss: 0.9307, Train Acc: 0.9837, Val Loss: 1.2790, Val Acc: 0.9290\n",
      "Epoch 156/1000000: LR: 0.000000, Train Loss: 1.0477, Train Acc: 0.9802, Val Loss: 1.2549, Val Acc: 0.9251\n",
      "Epoch 157/1000000: LR: 0.000000, Train Loss: 1.0674, Train Acc: 0.9802, Val Loss: 1.2521, Val Acc: 0.9251\n",
      "Epoch 158/1000000: LR: 0.000000, Train Loss: 1.3507, Train Acc: 0.9761, Val Loss: 1.2063, Val Acc: 0.9251\n",
      "Epoch 159/1000000: LR: 0.000000, Train Loss: 1.3801, Train Acc: 0.9674, Val Loss: 1.2612, Val Acc: 0.9271\n",
      "Epoch 160/1000000: LR: 0.000000, Train Loss: 0.8966, Train Acc: 0.9866, Val Loss: 1.2650, Val Acc: 0.9271\n",
      "Epoch 161/1000000: LR: 0.000000, Train Loss: 0.9719, Train Acc: 0.9854, Val Loss: 1.2609, Val Acc: 0.9251\n",
      "Epoch 162/1000000: LR: 0.000000, Train Loss: 0.8594, Train Acc: 0.9901, Val Loss: 1.2250, Val Acc: 0.9290\n",
      "Epoch 163/1000000: LR: 0.000000, Train Loss: 2.0432, Train Acc: 0.9610, Val Loss: 1.2604, Val Acc: 0.9328\n",
      "Epoch 164/1000000: LR: 0.000000, Train Loss: 1.6805, Train Acc: 0.9732, Val Loss: 1.2389, Val Acc: 0.9290\n",
      "Epoch 165/1000000: LR: 0.000000, Train Loss: 1.1190, Train Acc: 0.9779, Val Loss: 1.2080, Val Acc: 0.9290\n",
      "Epoch 166/1000000: LR: 0.000000, Train Loss: 1.1575, Train Acc: 0.9802, Val Loss: 1.2364, Val Acc: 0.9328\n",
      "Epoch 167/1000000: LR: 0.000000, Train Loss: 1.0740, Train Acc: 0.9825, Val Loss: 1.2195, Val Acc: 0.9290\n",
      "Epoch 168/1000000: LR: 0.000000, Train Loss: 0.9866, Train Acc: 0.9819, Val Loss: 1.2180, Val Acc: 0.9290\n",
      "Epoch 169/1000000: LR: 0.000000, Train Loss: 1.0721, Train Acc: 0.9814, Val Loss: 1.2941, Val Acc: 0.9271\n",
      "Epoch 170/1000000: LR: 0.000000, Train Loss: 2.0761, Train Acc: 0.9656, Val Loss: 1.2097, Val Acc: 0.9251\n",
      "Epoch 171/1000000: LR: 0.000000, Train Loss: 1.1410, Train Acc: 0.9755, Val Loss: 1.2378, Val Acc: 0.9251\n",
      "Epoch 172/1000000: LR: 0.000000, Train Loss: 1.4036, Train Acc: 0.9726, Val Loss: 1.2775, Val Acc: 0.9194\n",
      "Epoch 173/1000000: LR: 0.000000, Train Loss: 1.2934, Train Acc: 0.9732, Val Loss: 1.2742, Val Acc: 0.9271\n",
      "Epoch 174/1000000: LR: 0.000000, Train Loss: 1.1509, Train Acc: 0.9802, Val Loss: 1.2991, Val Acc: 0.9271\n",
      "Epoch 175/1000000: LR: 0.000000, Train Loss: 1.0890, Train Acc: 0.9773, Val Loss: 1.2527, Val Acc: 0.9251\n",
      "Epoch 176/1000000: LR: 0.000000, Train Loss: 0.9052, Train Acc: 0.9878, Val Loss: 1.2679, Val Acc: 0.9271\n",
      "Epoch 177/1000000: LR: 0.000000, Train Loss: 1.0809, Train Acc: 0.9837, Val Loss: 1.2148, Val Acc: 0.9309\n",
      "Epoch 178/1000000: LR: 0.000000, Train Loss: 0.9327, Train Acc: 0.9819, Val Loss: 1.2890, Val Acc: 0.9271\n",
      "Epoch 179/1000000: LR: 0.000000, Train Loss: 1.1967, Train Acc: 0.9843, Val Loss: 1.2542, Val Acc: 0.9271\n",
      "Epoch 180/1000000: LR: 0.000000, Train Loss: 1.2491, Train Acc: 0.9767, Val Loss: 1.2470, Val Acc: 0.9271\n",
      "Epoch 181/1000000: LR: 0.000000, Train Loss: 1.0607, Train Acc: 0.9796, Val Loss: 1.2617, Val Acc: 0.9271\n",
      "Epoch 182/1000000: LR: 0.000000, Train Loss: 1.2746, Train Acc: 0.9732, Val Loss: 1.2637, Val Acc: 0.9367\n",
      "Epoch 183/1000000: LR: 0.000000, Train Loss: 1.1615, Train Acc: 0.9814, Val Loss: 1.2200, Val Acc: 0.9328\n",
      "Epoch 184/1000000: LR: 0.000000, Train Loss: 1.7726, Train Acc: 0.9528, Val Loss: 1.2120, Val Acc: 0.9347\n",
      "Epoch 185/1000000: LR: 0.000000, Train Loss: 1.1343, Train Acc: 0.9784, Val Loss: 1.2967, Val Acc: 0.9232\n",
      "Epoch 186/1000000: LR: 0.000000, Train Loss: 1.4375, Train Acc: 0.9709, Val Loss: 1.2579, Val Acc: 0.9367\n",
      "Epoch 187/1000000: LR: 0.000000, Train Loss: 1.4542, Train Acc: 0.9819, Val Loss: 1.3033, Val Acc: 0.9328\n",
      "Epoch 188/1000000: LR: 0.000000, Train Loss: 1.4763, Train Acc: 0.9685, Val Loss: 1.2751, Val Acc: 0.9251\n",
      "Epoch 189/1000000: LR: 0.000000, Train Loss: 1.0198, Train Acc: 0.9843, Val Loss: 1.3000, Val Acc: 0.9232\n",
      "Epoch 190/1000000: LR: 0.000000, Train Loss: 1.3661, Train Acc: 0.9726, Val Loss: 1.2645, Val Acc: 0.9290\n",
      "Epoch 191/1000000: LR: 0.000000, Train Loss: 1.3410, Train Acc: 0.9796, Val Loss: 1.2434, Val Acc: 0.9271\n",
      "Epoch 192/1000000: LR: 0.000000, Train Loss: 0.8003, Train Acc: 0.9924, Val Loss: 1.2308, Val Acc: 0.9290\n",
      "Epoch 193/1000000: LR: 0.000000, Train Loss: 2.1798, Train Acc: 0.9476, Val Loss: 1.2435, Val Acc: 0.9232\n",
      "Epoch 194/1000000: LR: 0.000000, Train Loss: 1.1369, Train Acc: 0.9802, Val Loss: 1.2502, Val Acc: 0.9271\n",
      "Epoch 195/1000000: LR: 0.000000, Train Loss: 0.8902, Train Acc: 0.9883, Val Loss: 1.2493, Val Acc: 0.9271\n",
      "Epoch 196/1000000: LR: 0.000000, Train Loss: 1.1086, Train Acc: 0.9819, Val Loss: 1.1986, Val Acc: 0.9347\n",
      "Epoch 197/1000000: LR: 0.000000, Train Loss: 1.1055, Train Acc: 0.9755, Val Loss: 1.2329, Val Acc: 0.9232\n",
      "Epoch 198/1000000: LR: 0.000000, Train Loss: 0.9803, Train Acc: 0.9878, Val Loss: 1.2091, Val Acc: 0.9347\n",
      "Epoch 199/1000000: LR: 0.000000, Train Loss: 1.0425, Train Acc: 0.9854, Val Loss: 1.2089, Val Acc: 0.9328\n",
      "Epoch 200/1000000: LR: 0.000000, Train Loss: 1.0662, Train Acc: 0.9814, Val Loss: 1.2566, Val Acc: 0.9271\n",
      "Epoch 201/1000000: LR: 0.000000, Train Loss: 2.2677, Train Acc: 0.9714, Val Loss: 1.2915, Val Acc: 0.9271\n",
      "Epoch 202/1000000: LR: 0.000000, Train Loss: 1.0495, Train Acc: 0.9843, Val Loss: 1.2499, Val Acc: 0.9290\n",
      "Epoch 203/1000000: LR: 0.000000, Train Loss: 1.7436, Train Acc: 0.9650, Val Loss: 1.2036, Val Acc: 0.9290\n",
      "Epoch 204/1000000: LR: 0.000000, Train Loss: 1.0183, Train Acc: 0.9854, Val Loss: 1.2582, Val Acc: 0.9367\n",
      "Epoch 205/1000000: LR: 0.000000, Train Loss: 1.6502, Train Acc: 0.9545, Val Loss: 1.2242, Val Acc: 0.9290\n",
      "Epoch 206/1000000: LR: 0.000000, Train Loss: 1.8713, Train Acc: 0.9551, Val Loss: 1.2247, Val Acc: 0.9271\n",
      "Epoch 207/1000000: LR: 0.000000, Train Loss: 1.2320, Train Acc: 0.9726, Val Loss: 1.2565, Val Acc: 0.9367\n",
      "Epoch 208/1000000: LR: 0.000000, Train Loss: 0.9548, Train Acc: 0.9860, Val Loss: 1.3013, Val Acc: 0.9251\n",
      "Epoch 209/1000000: LR: 0.000000, Train Loss: 0.9813, Train Acc: 0.9889, Val Loss: 1.2946, Val Acc: 0.9232\n",
      "Epoch 210/1000000: LR: 0.000000, Train Loss: 1.1466, Train Acc: 0.9825, Val Loss: 1.2522, Val Acc: 0.9367\n",
      "Epoch 211/1000000: LR: 0.000000, Train Loss: 1.2462, Train Acc: 0.9726, Val Loss: 1.2101, Val Acc: 0.9271\n",
      "Epoch 212/1000000: LR: 0.000000, Train Loss: 1.1285, Train Acc: 0.9779, Val Loss: 1.2983, Val Acc: 0.9309\n",
      "Epoch 213/1000000: LR: 0.000000, Train Loss: 1.1106, Train Acc: 0.9808, Val Loss: 1.2364, Val Acc: 0.9271\n",
      "Epoch 214/1000000: LR: 0.000000, Train Loss: 1.4232, Train Acc: 0.9726, Val Loss: 1.2609, Val Acc: 0.9328\n",
      "Epoch 215/1000000: LR: 0.000000, Train Loss: 0.9497, Train Acc: 0.9831, Val Loss: 1.2436, Val Acc: 0.9271\n",
      "Epoch 216/1000000: LR: 0.000000, Train Loss: 1.0707, Train Acc: 0.9808, Val Loss: 1.2312, Val Acc: 0.9309\n",
      "Epoch 217/1000000: LR: 0.000000, Train Loss: 0.8748, Train Acc: 0.9848, Val Loss: 1.3099, Val Acc: 0.9271\n",
      "Epoch 218/1000000: LR: 0.000000, Train Loss: 1.0687, Train Acc: 0.9831, Val Loss: 1.2768, Val Acc: 0.9271\n",
      "Epoch 219/1000000: LR: 0.000000, Train Loss: 0.9839, Train Acc: 0.9860, Val Loss: 1.2668, Val Acc: 0.9251\n",
      "Epoch 220/1000000: LR: 0.000000, Train Loss: 1.0551, Train Acc: 0.9814, Val Loss: 1.2584, Val Acc: 0.9251\n",
      "Epoch 221/1000000: LR: 0.000000, Train Loss: 0.9056, Train Acc: 0.9866, Val Loss: 1.2465, Val Acc: 0.9251\n",
      "Epoch 222/1000000: LR: 0.000000, Train Loss: 1.1244, Train Acc: 0.9808, Val Loss: 1.2281, Val Acc: 0.9271\n",
      "Epoch 223/1000000: LR: 0.000000, Train Loss: 1.3933, Train Acc: 0.9668, Val Loss: 1.2601, Val Acc: 0.9367\n",
      "Epoch 224/1000000: LR: 0.000000, Train Loss: 1.2436, Train Acc: 0.9773, Val Loss: 1.2543, Val Acc: 0.9367\n",
      "Epoch 225/1000000: LR: 0.000000, Train Loss: 1.1960, Train Acc: 0.9720, Val Loss: 1.1991, Val Acc: 0.9328\n",
      "Epoch 226/1000000: LR: 0.000000, Train Loss: 1.6258, Train Acc: 0.9650, Val Loss: 1.2916, Val Acc: 0.9194\n",
      "Epoch 227/1000000: LR: 0.000000, Train Loss: 1.4791, Train Acc: 0.9802, Val Loss: 1.3553, Val Acc: 0.9136\n",
      "Epoch 228/1000000: LR: 0.000000, Train Loss: 1.2096, Train Acc: 0.9802, Val Loss: 1.2366, Val Acc: 0.9328\n",
      "Epoch 229/1000000: LR: 0.000000, Train Loss: 1.0908, Train Acc: 0.9831, Val Loss: 1.2443, Val Acc: 0.9271\n",
      "Epoch 230/1000000: LR: 0.000000, Train Loss: 0.9655, Train Acc: 0.9831, Val Loss: 1.2677, Val Acc: 0.9290\n",
      "Epoch 231/1000000: LR: 0.000000, Train Loss: 1.3235, Train Acc: 0.9714, Val Loss: 1.2061, Val Acc: 0.9405\n",
      "Epoch 232/1000000: LR: 0.000000, Train Loss: 1.1748, Train Acc: 0.9866, Val Loss: 1.2753, Val Acc: 0.9290\n",
      "Epoch 233/1000000: LR: 0.000000, Train Loss: 1.2718, Train Acc: 0.9679, Val Loss: 1.2197, Val Acc: 0.9232\n",
      "Epoch 234/1000000: LR: 0.000000, Train Loss: 1.2635, Train Acc: 0.9738, Val Loss: 1.3049, Val Acc: 0.9251\n",
      "Epoch 235/1000000: LR: 0.000000, Train Loss: 1.0020, Train Acc: 0.9883, Val Loss: 1.2979, Val Acc: 0.9271\n",
      "Epoch 236/1000000: LR: 0.000000, Train Loss: 1.7938, Train Acc: 0.9580, Val Loss: 1.2698, Val Acc: 0.9232\n",
      "Epoch 237/1000000: LR: 0.000000, Train Loss: 1.4279, Train Acc: 0.9744, Val Loss: 1.3206, Val Acc: 0.9251\n",
      "Epoch 238/1000000: LR: 0.000000, Train Loss: 1.2934, Train Acc: 0.9773, Val Loss: 1.2219, Val Acc: 0.9251\n",
      "Epoch 239/1000000: LR: 0.000000, Train Loss: 0.7918, Train Acc: 0.9883, Val Loss: 1.2549, Val Acc: 0.9251\n",
      "Epoch 240/1000000: LR: 0.000000, Train Loss: 1.3377, Train Acc: 0.9726, Val Loss: 1.2692, Val Acc: 0.9232\n",
      "Epoch 241/1000000: LR: 0.000000, Train Loss: 0.8794, Train Acc: 0.9848, Val Loss: 1.2462, Val Acc: 0.9213\n",
      "Epoch 242/1000000: LR: 0.000000, Train Loss: 1.7714, Train Acc: 0.9575, Val Loss: 1.2327, Val Acc: 0.9251\n",
      "Epoch 243/1000000: LR: 0.000000, Train Loss: 0.9115, Train Acc: 0.9860, Val Loss: 1.2369, Val Acc: 0.9232\n",
      "Epoch 244/1000000: LR: 0.000000, Train Loss: 1.0578, Train Acc: 0.9796, Val Loss: 1.2852, Val Acc: 0.9309\n",
      "Epoch 245/1000000: LR: 0.000000, Train Loss: 1.1823, Train Acc: 0.9732, Val Loss: 1.2750, Val Acc: 0.9290\n",
      "Epoch 246/1000000: LR: 0.000000, Train Loss: 1.4454, Train Acc: 0.9714, Val Loss: 1.2709, Val Acc: 0.9251\n",
      "Epoch 247/1000000: LR: 0.000000, Train Loss: 1.2484, Train Acc: 0.9784, Val Loss: 1.2721, Val Acc: 0.9251\n",
      "Epoch 248/1000000: LR: 0.000000, Train Loss: 1.0783, Train Acc: 0.9814, Val Loss: 1.2465, Val Acc: 0.9271\n",
      "Epoch 249/1000000: LR: 0.000000, Train Loss: 0.8851, Train Acc: 0.9901, Val Loss: 1.2216, Val Acc: 0.9251\n",
      "Epoch 250/1000000: LR: 0.000000, Train Loss: 1.3029, Train Acc: 0.9755, Val Loss: 1.2415, Val Acc: 0.9347\n",
      "Epoch 251/1000000: LR: 0.000000, Train Loss: 1.0628, Train Acc: 0.9848, Val Loss: 1.2470, Val Acc: 0.9271\n",
      "Epoch 252/1000000: LR: 0.000000, Train Loss: 1.2302, Train Acc: 0.9784, Val Loss: 1.2048, Val Acc: 0.9328\n",
      "Epoch 253/1000000: LR: 0.000000, Train Loss: 0.9350, Train Acc: 0.9872, Val Loss: 1.2031, Val Acc: 0.9309\n",
      "Epoch 254/1000000: LR: 0.000000, Train Loss: 1.8068, Train Acc: 0.9551, Val Loss: 1.2228, Val Acc: 0.9251\n",
      "Epoch 255/1000000: LR: 0.000000, Train Loss: 1.0480, Train Acc: 0.9779, Val Loss: 1.2677, Val Acc: 0.9251\n",
      "Epoch 256/1000000: LR: 0.000000, Train Loss: 1.0501, Train Acc: 0.9848, Val Loss: 1.2597, Val Acc: 0.9232\n",
      "Epoch 257/1000000: LR: 0.000000, Train Loss: 1.4083, Train Acc: 0.9691, Val Loss: 1.2767, Val Acc: 0.9232\n",
      "Epoch 258/1000000: LR: 0.000000, Train Loss: 0.8715, Train Acc: 0.9866, Val Loss: 1.3087, Val Acc: 0.9290\n",
      "Epoch 259/1000000: LR: 0.000000, Train Loss: 0.8452, Train Acc: 0.9872, Val Loss: 1.2609, Val Acc: 0.9271\n",
      "Epoch 260/1000000: LR: 0.000000, Train Loss: 1.2443, Train Acc: 0.9779, Val Loss: 1.2089, Val Acc: 0.9271\n",
      "Epoch 261/1000000: LR: 0.000000, Train Loss: 0.8744, Train Acc: 0.9854, Val Loss: 1.2058, Val Acc: 0.9271\n",
      "Epoch 262/1000000: LR: 0.000000, Train Loss: 1.1843, Train Acc: 0.9779, Val Loss: 1.2770, Val Acc: 0.9271\n",
      "Epoch 263/1000000: LR: 0.000000, Train Loss: 0.9294, Train Acc: 0.9889, Val Loss: 1.2839, Val Acc: 0.9251\n",
      "Epoch 264/1000000: LR: 0.000000, Train Loss: 1.2584, Train Acc: 0.9779, Val Loss: 1.2574, Val Acc: 0.9271\n",
      "Epoch 265/1000000: LR: 0.000000, Train Loss: 1.1779, Train Acc: 0.9796, Val Loss: 1.2422, Val Acc: 0.9251\n",
      "Epoch 266/1000000: LR: 0.000000, Train Loss: 1.4811, Train Acc: 0.9674, Val Loss: 1.2288, Val Acc: 0.9232\n",
      "Epoch 267/1000000: LR: 0.000000, Train Loss: 1.3716, Train Acc: 0.9773, Val Loss: 1.3199, Val Acc: 0.9271\n",
      "Epoch 268/1000000: LR: 0.000000, Train Loss: 1.1460, Train Acc: 0.9726, Val Loss: 1.2900, Val Acc: 0.9271\n",
      "Epoch 269/1000000: LR: 0.000000, Train Loss: 1.2463, Train Acc: 0.9761, Val Loss: 1.2870, Val Acc: 0.9271\n",
      "Epoch 270/1000000: LR: 0.000000, Train Loss: 1.1108, Train Acc: 0.9767, Val Loss: 1.2612, Val Acc: 0.9251\n",
      "Epoch 271/1000000: LR: 0.000000, Train Loss: 1.5379, Train Acc: 0.9668, Val Loss: 1.2338, Val Acc: 0.9271\n",
      "Epoch 272/1000000: LR: 0.000000, Train Loss: 1.0586, Train Acc: 0.9814, Val Loss: 1.3025, Val Acc: 0.9290\n",
      "Epoch 273/1000000: LR: 0.000000, Train Loss: 2.8077, Train Acc: 0.9376, Val Loss: 1.2867, Val Acc: 0.9271\n",
      "Epoch 274/1000000: LR: 0.000000, Train Loss: 1.6374, Train Acc: 0.9767, Val Loss: 1.2353, Val Acc: 0.9251\n",
      "Epoch 275/1000000: LR: 0.000000, Train Loss: 0.8903, Train Acc: 0.9854, Val Loss: 1.2744, Val Acc: 0.9251\n",
      "Epoch 276/1000000: LR: 0.000000, Train Loss: 1.1564, Train Acc: 0.9755, Val Loss: 1.2901, Val Acc: 0.9271\n",
      "Epoch 277/1000000: LR: 0.000000, Train Loss: 1.0919, Train Acc: 0.9814, Val Loss: 1.2719, Val Acc: 0.9290\n",
      "Epoch 278/1000000: LR: 0.000000, Train Loss: 1.1924, Train Acc: 0.9796, Val Loss: 1.2651, Val Acc: 0.9232\n",
      "Epoch 279/1000000: LR: 0.000000, Train Loss: 1.8686, Train Acc: 0.9726, Val Loss: 1.2205, Val Acc: 0.9251\n",
      "Epoch 280/1000000: LR: 0.000000, Train Loss: 1.1407, Train Acc: 0.9814, Val Loss: 1.2941, Val Acc: 0.9251\n",
      "Epoch 281/1000000: LR: 0.000000, Train Loss: 1.1711, Train Acc: 0.9796, Val Loss: 1.2978, Val Acc: 0.9251\n",
      "Epoch 282/1000000: LR: 0.000000, Train Loss: 0.8602, Train Acc: 0.9907, Val Loss: 1.3113, Val Acc: 0.9251\n",
      "Epoch 283/1000000: LR: 0.000000, Train Loss: 1.2197, Train Acc: 0.9773, Val Loss: 1.2819, Val Acc: 0.9232\n",
      "Epoch 284/1000000: LR: 0.000000, Train Loss: 1.6144, Train Acc: 0.9639, Val Loss: 1.2839, Val Acc: 0.9232\n",
      "Epoch 285/1000000: LR: 0.000000, Train Loss: 1.0114, Train Acc: 0.9831, Val Loss: 1.2273, Val Acc: 0.9251\n",
      "Epoch 286/1000000: LR: 0.000000, Train Loss: 1.0153, Train Acc: 0.9796, Val Loss: 1.2339, Val Acc: 0.9251\n",
      "Epoch 287/1000000: LR: 0.000000, Train Loss: 1.8802, Train Acc: 0.9697, Val Loss: 1.2831, Val Acc: 0.9271\n",
      "Epoch 288/1000000: LR: 0.000000, Train Loss: 0.8276, Train Acc: 0.9901, Val Loss: 1.2525, Val Acc: 0.9251\n",
      "Epoch 289/1000000: LR: 0.000000, Train Loss: 1.1797, Train Acc: 0.9790, Val Loss: 1.2593, Val Acc: 0.9251\n",
      "Epoch 290/1000000: LR: 0.000000, Train Loss: 0.9629, Train Acc: 0.9843, Val Loss: 1.2716, Val Acc: 0.9251\n",
      "Epoch 291/1000000: LR: 0.000000, Train Loss: 1.0040, Train Acc: 0.9808, Val Loss: 1.2621, Val Acc: 0.9251\n",
      "Epoch 292/1000000: LR: 0.000000, Train Loss: 1.5394, Train Acc: 0.9767, Val Loss: 1.2720, Val Acc: 0.9309\n",
      "Epoch 293/1000000: LR: 0.000000, Train Loss: 1.0633, Train Acc: 0.9825, Val Loss: 1.2495, Val Acc: 0.9251\n",
      "Epoch 294/1000000: LR: 0.000000, Train Loss: 1.8774, Train Acc: 0.9645, Val Loss: 1.2217, Val Acc: 0.9251\n",
      "Epoch 295/1000000: LR: 0.000000, Train Loss: 1.4326, Train Acc: 0.9755, Val Loss: 1.2722, Val Acc: 0.9290\n",
      "Epoch 296/1000000: LR: 0.000000, Train Loss: 1.3816, Train Acc: 0.9738, Val Loss: 1.2464, Val Acc: 0.9251\n",
      "Epoch 297/1000000: LR: 0.000000, Train Loss: 2.2960, Train Acc: 0.9493, Val Loss: 1.2639, Val Acc: 0.9367\n",
      "Epoch 298/1000000: LR: 0.000000, Train Loss: 1.1775, Train Acc: 0.9819, Val Loss: 1.2863, Val Acc: 0.9309\n",
      "Epoch 299/1000000: LR: 0.000000, Train Loss: 0.9917, Train Acc: 0.9843, Val Loss: 1.2470, Val Acc: 0.9251\n",
      "Epoch 300/1000000: LR: 0.000000, Train Loss: 0.9499, Train Acc: 0.9883, Val Loss: 1.2333, Val Acc: 0.9271\n",
      "Epoch 301/1000000: LR: 0.000000, Train Loss: 1.2025, Train Acc: 0.9767, Val Loss: 1.2346, Val Acc: 0.9232\n",
      "Epoch 302/1000000: LR: 0.000000, Train Loss: 1.5824, Train Acc: 0.9610, Val Loss: 1.2353, Val Acc: 0.9328\n",
      "Epoch 303/1000000: LR: 0.000000, Train Loss: 1.1321, Train Acc: 0.9790, Val Loss: 1.2439, Val Acc: 0.9232\n",
      "Epoch 304/1000000: LR: 0.000000, Train Loss: 0.9239, Train Acc: 0.9866, Val Loss: 1.2540, Val Acc: 0.9251\n",
      "Epoch 305/1000000: LR: 0.000000, Train Loss: 1.2126, Train Acc: 0.9784, Val Loss: 1.2887, Val Acc: 0.9251\n",
      "Epoch 306/1000000: LR: 0.000000, Train Loss: 0.8944, Train Acc: 0.9883, Val Loss: 1.2437, Val Acc: 0.9271\n",
      "Epoch 307/1000000: LR: 0.000000, Train Loss: 1.5419, Train Acc: 0.9674, Val Loss: 1.2379, Val Acc: 0.9367\n",
      "Epoch 308/1000000: LR: 0.000000, Train Loss: 0.9430, Train Acc: 0.9825, Val Loss: 1.2425, Val Acc: 0.9232\n",
      "Epoch 309/1000000: LR: 0.000000, Train Loss: 1.1408, Train Acc: 0.9796, Val Loss: 1.2123, Val Acc: 0.9251\n",
      "Epoch 310/1000000: LR: 0.000000, Train Loss: 1.2242, Train Acc: 0.9767, Val Loss: 1.2044, Val Acc: 0.9251\n",
      "🛑 Early stopping at epoch 310\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"C:\\Users\\ChoiSeongHyeon\\Desktop\\Dataset\\CWRU\\Data Division\"\n",
    "\n",
    "# Fault type labels\n",
    "fault_types = {'N': 0, 'IR': 1, 'OR@06': 2, 'B': 3}\n",
    "\n",
    "# Load dataset\n",
    "class CWRUDataset(Dataset):\n",
    "    def __init__(self, dataset_type=Dataset):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for fault, label in fault_types.items():\n",
    "            sample_path = os.path.join(base_path, fault, \"Samples\")\n",
    "\n",
    "            if not os.path.exists(sample_path):\n",
    "                print(f\"Warning: Path does not exist - {sample_path}\")\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(sample_path):\n",
    "                if file.endswith('.csv') and dataset_type in file:\n",
    "                    file_path = os.path.join(sample_path, file)\n",
    "                    data = pd.read_csv(file_path, header=None).values.flatten()\n",
    "\n",
    "                    # Ensure fixed sample size (4096)\n",
    "                    if len(data) == 4096:\n",
    "                        self.data.append(data)\n",
    "                        self.labels.append(label)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32).unsqueeze(1)  # (N, 1, 4096)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "        # 🔹 \"Validation\"과 \"Test\" 데이터는 한 번만 셔플 (seed 고정)\n",
    "        if dataset_type in [\"Validation\", \"Test\"]:\n",
    "            np.random.seed(42)  # 실행할 때마다 동일한 shuffle 유지\n",
    "            indices = np.random.permutation(len(self.data))\n",
    "            self.data = self.data[indices]\n",
    "            self.labels = self.labels[indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Define WDCNN Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WDCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(WDCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=64, stride=16, padding=24)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 🔹 마지막 풀링을 Global Max Pooling으로 변경\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # 🔹 Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(64, 100)\n",
    "        self.dropout = nn.Dropout(0.2)  # 🔹 Dropout 추가\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.global_max_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # 🔹 훈련 시 Dropout 적용\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Training function with Early Stopping, Learning Rate Scheduler, and LR Output\n",
    "def train_model(model, train_loader, val_loader, num_epochs=1000000, learning_rate=0.1, patience=300):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # 🔹 현재 Learning Rate 출력\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: LR: {current_lr:.6f}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping & Model Saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            early_stop_counter = 0\n",
    "            torch.save({'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'learning_rate': current_lr},\n",
    "                       \"best_model.pth\")\n",
    "            print(f\"✅ Best model saved at epoch {epoch+1} with val_acc {best_val_acc:.4f}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"🛑 Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_dataset = CWRUDataset(\"Train\")\n",
    "val_dataset = CWRUDataset(\"Validation\")\n",
    "test_dataset = CWRUDataset(\"Test\")\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "model = WDCNN()\n",
    "train_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training with Batch Size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChoiSeongHyeon\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Batch 32, LR: 0.100000, Train Acc: 0.4802, Val Acc: 0.4107\n",
      "✅ Best model saved (Batch 32) with val_acc 0.4107\n",
      "Epoch 2: Batch 32, LR: 0.100000, Train Acc: 0.6090, Val Acc: 0.7063\n",
      "✅ Best model saved (Batch 32) with val_acc 0.7063\n",
      "Epoch 3: Batch 32, LR: 0.100000, Train Acc: 0.7069, Val Acc: 0.8349\n",
      "✅ Best model saved (Batch 32) with val_acc 0.8349\n",
      "Epoch 4: Batch 32, LR: 0.100000, Train Acc: 0.7488, Val Acc: 0.8714\n",
      "✅ Best model saved (Batch 32) with val_acc 0.8714\n",
      "Epoch 5: Batch 32, LR: 0.100000, Train Acc: 0.7086, Val Acc: 0.5393\n",
      "Epoch 6: Batch 32, LR: 0.100000, Train Acc: 0.7442, Val Acc: 0.8157\n",
      "Epoch 7: Batch 32, LR: 0.100000, Train Acc: 0.7529, Val Acc: 0.7390\n",
      "Epoch 8: Batch 32, LR: 0.100000, Train Acc: 0.7500, Val Acc: 0.8215\n",
      "Epoch 9: Batch 32, LR: 0.100000, Train Acc: 0.7308, Val Acc: 0.8618\n",
      "Epoch 10: Batch 32, LR: 0.100000, Train Acc: 0.7558, Val Acc: 0.8061\n",
      "Epoch 11: Batch 32, LR: 0.100000, Train Acc: 0.7605, Val Acc: 0.8541\n",
      "Epoch 12: Batch 32, LR: 0.100000, Train Acc: 0.7686, Val Acc: 0.7678\n",
      "Epoch 13: Batch 32, LR: 0.100000, Train Acc: 0.7716, Val Acc: 0.8637\n",
      "Epoch 14: Batch 32, LR: 0.100000, Train Acc: 0.7791, Val Acc: 0.8637\n",
      "Epoch 15: Batch 32, LR: 0.100000, Train Acc: 0.8036, Val Acc: 0.8484\n",
      "Epoch 16: Batch 32, LR: 0.100000, Train Acc: 0.6929, Val Acc: 0.7179\n",
      "Epoch 17: Batch 32, LR: 0.100000, Train Acc: 0.7150, Val Acc: 0.7486\n",
      "Epoch 18: Batch 32, LR: 0.100000, Train Acc: 0.6935, Val Acc: 0.7486\n",
      "Epoch 19: Batch 32, LR: 0.100000, Train Acc: 0.6981, Val Acc: 0.7370\n",
      "Epoch 20: Batch 32, LR: 0.100000, Train Acc: 0.6684, Val Acc: 0.7524\n",
      "Epoch 21: Batch 32, LR: 0.100000, Train Acc: 0.6772, Val Acc: 0.7121\n",
      "Epoch 22: Batch 32, LR: 0.100000, Train Acc: 0.6981, Val Acc: 0.7620\n",
      "Epoch 23: Batch 32, LR: 0.100000, Train Acc: 0.6941, Val Acc: 0.7697\n",
      "Epoch 24: Batch 32, LR: 0.100000, Train Acc: 0.6807, Val Acc: 0.7121\n",
      "Epoch 25: Batch 32, LR: 0.100000, Train Acc: 0.6812, Val Acc: 0.7543\n",
      "Epoch 26: Batch 32, LR: 0.100000, Train Acc: 0.6865, Val Acc: 0.7447\n",
      "Epoch 27: Batch 32, LR: 0.010000, Train Acc: 0.7075, Val Acc: 0.7428\n",
      "Epoch 28: Batch 32, LR: 0.010000, Train Acc: 0.6946, Val Acc: 0.7908\n",
      "Epoch 29: Batch 32, LR: 0.010000, Train Acc: 0.7162, Val Acc: 0.7620\n",
      "Epoch 30: Batch 32, LR: 0.010000, Train Acc: 0.7337, Val Acc: 0.7908\n",
      "Epoch 31: Batch 32, LR: 0.010000, Train Acc: 0.7424, Val Acc: 0.7869\n",
      "Epoch 32: Batch 32, LR: 0.010000, Train Acc: 0.7494, Val Acc: 0.7869\n",
      "Epoch 33: Batch 32, LR: 0.010000, Train Acc: 0.7005, Val Acc: 0.7658\n",
      "Epoch 34: Batch 32, LR: 0.010000, Train Acc: 0.7249, Val Acc: 0.7716\n",
      "🛑 Early stopping at epoch 34\n",
      "\n",
      "🏆 New Best Overall Model Saved! Val Acc: 0.8714 (Batch 32)\n",
      "\n",
      "🔹 Training with Batch Size: 64\n",
      "Epoch 1: Batch 64, LR: 0.100000, Train Acc: 0.4563, Val Acc: 0.5374\n",
      "✅ Best model saved (Batch 64) with val_acc 0.5374\n",
      "Epoch 2: Batch 64, LR: 0.100000, Train Acc: 0.6847, Val Acc: 0.8119\n",
      "✅ Best model saved (Batch 64) with val_acc 0.8119\n",
      "Epoch 3: Batch 64, LR: 0.100000, Train Acc: 0.7296, Val Acc: 0.7121\n",
      "Epoch 4: Batch 64, LR: 0.100000, Train Acc: 0.7500, Val Acc: 0.7927\n",
      "Epoch 5: Batch 64, LR: 0.100000, Train Acc: 0.8048, Val Acc: 0.8100\n",
      "Epoch 6: Batch 64, LR: 0.100000, Train Acc: 0.7966, Val Acc: 0.8522\n",
      "✅ Best model saved (Batch 64) with val_acc 0.8522\n",
      "Epoch 7: Batch 64, LR: 0.100000, Train Acc: 0.8531, Val Acc: 0.8964\n",
      "✅ Best model saved (Batch 64) with val_acc 0.8964\n",
      "Epoch 8: Batch 64, LR: 0.100000, Train Acc: 0.8129, Val Acc: 0.5701\n",
      "Epoch 9: Batch 64, LR: 0.100000, Train Acc: 0.8100, Val Acc: 0.8292\n",
      "Epoch 10: Batch 64, LR: 0.100000, Train Acc: 0.8368, Val Acc: 0.8791\n",
      "Epoch 11: Batch 64, LR: 0.100000, Train Acc: 0.8811, Val Acc: 0.8944\n",
      "Epoch 12: Batch 64, LR: 0.100000, Train Acc: 0.8246, Val Acc: 0.8426\n",
      "Epoch 13: Batch 64, LR: 0.100000, Train Acc: 0.8182, Val Acc: 0.5202\n",
      "Epoch 14: Batch 64, LR: 0.100000, Train Acc: 0.8421, Val Acc: 0.8157\n",
      "Epoch 15: Batch 64, LR: 0.100000, Train Acc: 0.8700, Val Acc: 0.9060\n",
      "✅ Best model saved (Batch 64) with val_acc 0.9060\n",
      "Epoch 16: Batch 64, LR: 0.100000, Train Acc: 0.8631, Val Acc: 0.8369\n",
      "Epoch 17: Batch 64, LR: 0.100000, Train Acc: 0.8980, Val Acc: 0.9213\n",
      "✅ Best model saved (Batch 64) with val_acc 0.9213\n",
      "Epoch 18: Batch 64, LR: 0.100000, Train Acc: 0.8939, Val Acc: 0.9155\n",
      "Epoch 19: Batch 64, LR: 0.100000, Train Acc: 0.8765, Val Acc: 0.8177\n",
      "Epoch 20: Batch 64, LR: 0.100000, Train Acc: 0.8590, Val Acc: 0.8560\n",
      "Epoch 21: Batch 64, LR: 0.100000, Train Acc: 0.8502, Val Acc: 0.8330\n",
      "Epoch 22: Batch 64, LR: 0.100000, Train Acc: 0.8765, Val Acc: 0.8772\n",
      "Epoch 23: Batch 64, LR: 0.100000, Train Acc: 0.8910, Val Acc: 0.7774\n",
      "Epoch 24: Batch 64, LR: 0.100000, Train Acc: 0.8392, Val Acc: 0.8426\n",
      "Epoch 25: Batch 64, LR: 0.100000, Train Acc: 0.8555, Val Acc: 0.9290\n",
      "✅ Best model saved (Batch 64) with val_acc 0.9290\n",
      "Epoch 26: Batch 64, LR: 0.100000, Train Acc: 0.8840, Val Acc: 0.8714\n",
      "Epoch 27: Batch 64, LR: 0.100000, Train Acc: 0.8572, Val Acc: 0.9367\n",
      "✅ Best model saved (Batch 64) with val_acc 0.9367\n",
      "Epoch 28: Batch 64, LR: 0.100000, Train Acc: 0.9295, Val Acc: 0.9175\n",
      "Epoch 29: Batch 64, LR: 0.100000, Train Acc: 0.8840, Val Acc: 0.8349\n",
      "Epoch 30: Batch 64, LR: 0.100000, Train Acc: 0.8485, Val Acc: 0.8618\n",
      "Epoch 31: Batch 64, LR: 0.100000, Train Acc: 0.8613, Val Acc: 0.8964\n",
      "Epoch 32: Batch 64, LR: 0.100000, Train Acc: 0.9172, Val Acc: 0.9213\n",
      "Epoch 33: Batch 64, LR: 0.100000, Train Acc: 0.9231, Val Acc: 0.7812\n",
      "Epoch 34: Batch 64, LR: 0.100000, Train Acc: 0.9277, Val Acc: 0.9328\n",
      "Epoch 35: Batch 64, LR: 0.100000, Train Acc: 0.9120, Val Acc: 0.8752\n",
      "Epoch 36: Batch 64, LR: 0.100000, Train Acc: 0.8689, Val Acc: 0.9136\n",
      "Epoch 37: Batch 64, LR: 0.100000, Train Acc: 0.9038, Val Acc: 0.8714\n",
      "Epoch 38: Batch 64, LR: 0.100000, Train Acc: 0.9062, Val Acc: 0.8637\n",
      "Epoch 39: Batch 64, LR: 0.100000, Train Acc: 0.9190, Val Acc: 0.8445\n",
      "Epoch 40: Batch 64, LR: 0.100000, Train Acc: 0.9108, Val Acc: 0.9040\n",
      "Epoch 41: Batch 64, LR: 0.100000, Train Acc: 0.9009, Val Acc: 0.9155\n",
      "Epoch 42: Batch 64, LR: 0.100000, Train Acc: 0.8980, Val Acc: 0.8484\n",
      "Epoch 43: Batch 64, LR: 0.100000, Train Acc: 0.8788, Val Acc: 0.9232\n",
      "Epoch 44: Batch 64, LR: 0.010000, Train Acc: 0.8689, Val Acc: 0.9175\n",
      "Epoch 45: Batch 64, LR: 0.010000, Train Acc: 0.8939, Val Acc: 0.9463\n",
      "✅ Best model saved (Batch 64) with val_acc 0.9463\n",
      "Epoch 46: Batch 64, LR: 0.010000, Train Acc: 0.9190, Val Acc: 0.9367\n",
      "Epoch 47: Batch 64, LR: 0.010000, Train Acc: 0.9248, Val Acc: 0.9520\n",
      "✅ Best model saved (Batch 64) with val_acc 0.9520\n",
      "Epoch 48: Batch 64, LR: 0.010000, Train Acc: 0.9120, Val Acc: 0.9386\n",
      "Epoch 49: Batch 64, LR: 0.010000, Train Acc: 0.9254, Val Acc: 0.9290\n",
      "Epoch 50: Batch 64, LR: 0.010000, Train Acc: 0.9254, Val Acc: 0.9443\n",
      "Epoch 51: Batch 64, LR: 0.010000, Train Acc: 0.9394, Val Acc: 0.9309\n",
      "Epoch 52: Batch 64, LR: 0.010000, Train Acc: 0.9242, Val Acc: 0.9309\n",
      "Epoch 53: Batch 64, LR: 0.010000, Train Acc: 0.9318, Val Acc: 0.9232\n",
      "Epoch 54: Batch 64, LR: 0.010000, Train Acc: 0.9464, Val Acc: 0.9405\n",
      "Epoch 55: Batch 64, LR: 0.010000, Train Acc: 0.9516, Val Acc: 0.9463\n",
      "Epoch 56: Batch 64, LR: 0.010000, Train Acc: 0.9429, Val Acc: 0.9309\n",
      "Epoch 57: Batch 64, LR: 0.010000, Train Acc: 0.9505, Val Acc: 0.9443\n",
      "Epoch 58: Batch 64, LR: 0.010000, Train Acc: 0.9551, Val Acc: 0.9501\n",
      "Epoch 59: Batch 64, LR: 0.010000, Train Acc: 0.9586, Val Acc: 0.9386\n",
      "Epoch 60: Batch 64, LR: 0.010000, Train Acc: 0.9510, Val Acc: 0.9367\n",
      "Epoch 61: Batch 64, LR: 0.010000, Train Acc: 0.9569, Val Acc: 0.9424\n",
      "Epoch 62: Batch 64, LR: 0.010000, Train Acc: 0.9598, Val Acc: 0.9482\n",
      "Epoch 63: Batch 64, LR: 0.001000, Train Acc: 0.9615, Val Acc: 0.9463\n",
      "Epoch 64: Batch 64, LR: 0.001000, Train Acc: 0.9499, Val Acc: 0.9520\n",
      "Epoch 65: Batch 64, LR: 0.001000, Train Acc: 0.9516, Val Acc: 0.9501\n",
      "Epoch 66: Batch 64, LR: 0.001000, Train Acc: 0.9650, Val Acc: 0.9482\n",
      "Epoch 67: Batch 64, LR: 0.001000, Train Acc: 0.9580, Val Acc: 0.9501\n",
      "Epoch 68: Batch 64, LR: 0.001000, Train Acc: 0.9580, Val Acc: 0.9463\n",
      "Epoch 69: Batch 64, LR: 0.001000, Train Acc: 0.9534, Val Acc: 0.9520\n",
      "Epoch 70: Batch 64, LR: 0.001000, Train Acc: 0.9528, Val Acc: 0.9424\n",
      "Epoch 71: Batch 64, LR: 0.001000, Train Acc: 0.9470, Val Acc: 0.9405\n",
      "Epoch 72: Batch 64, LR: 0.001000, Train Acc: 0.9586, Val Acc: 0.9386\n",
      "Epoch 73: Batch 64, LR: 0.001000, Train Acc: 0.9580, Val Acc: 0.9443\n",
      "Epoch 74: Batch 64, LR: 0.001000, Train Acc: 0.9627, Val Acc: 0.9520\n",
      "Epoch 75: Batch 64, LR: 0.001000, Train Acc: 0.9627, Val Acc: 0.9386\n",
      "Epoch 76: Batch 64, LR: 0.001000, Train Acc: 0.9627, Val Acc: 0.9386\n",
      "Epoch 77: Batch 64, LR: 0.001000, Train Acc: 0.9639, Val Acc: 0.9482\n",
      "🛑 Early stopping at epoch 77\n",
      "\n",
      "🏆 New Best Overall Model Saved! Val Acc: 0.9520 (Batch 64)\n",
      "\n",
      "🔹 Training with Batch Size: 128\n",
      "Epoch 1: Batch 128, LR: 0.100000, Train Acc: 0.3613, Val Acc: 0.5336\n",
      "✅ Best model saved (Batch 128) with val_acc 0.5336\n",
      "Epoch 2: Batch 128, LR: 0.100000, Train Acc: 0.5606, Val Acc: 0.2994\n",
      "Epoch 3: Batch 128, LR: 0.100000, Train Acc: 0.6224, Val Acc: 0.3205\n",
      "Epoch 4: Batch 128, LR: 0.100000, Train Acc: 0.6043, Val Acc: 0.7562\n",
      "✅ Best model saved (Batch 128) with val_acc 0.7562\n",
      "Epoch 5: Batch 128, LR: 0.100000, Train Acc: 0.7552, Val Acc: 0.8061\n",
      "✅ Best model saved (Batch 128) with val_acc 0.8061\n",
      "Epoch 6: Batch 128, LR: 0.100000, Train Acc: 0.7716, Val Acc: 0.8292\n",
      "✅ Best model saved (Batch 128) with val_acc 0.8292\n",
      "Epoch 7: Batch 128, LR: 0.100000, Train Acc: 0.7949, Val Acc: 0.8503\n",
      "✅ Best model saved (Batch 128) with val_acc 0.8503\n",
      "Epoch 8: Batch 128, LR: 0.100000, Train Acc: 0.8159, Val Acc: 0.8234\n",
      "Epoch 9: Batch 128, LR: 0.100000, Train Acc: 0.8211, Val Acc: 0.8369\n",
      "Epoch 10: Batch 128, LR: 0.100000, Train Acc: 0.8170, Val Acc: 0.8273\n",
      "Epoch 11: Batch 128, LR: 0.100000, Train Acc: 0.8147, Val Acc: 0.9098\n",
      "✅ Best model saved (Batch 128) with val_acc 0.9098\n",
      "Epoch 12: Batch 128, LR: 0.100000, Train Acc: 0.8450, Val Acc: 0.8656\n",
      "Epoch 13: Batch 128, LR: 0.100000, Train Acc: 0.8147, Val Acc: 0.8253\n",
      "Epoch 14: Batch 128, LR: 0.100000, Train Acc: 0.8176, Val Acc: 0.8061\n",
      "Epoch 15: Batch 128, LR: 0.100000, Train Acc: 0.8427, Val Acc: 0.8791\n",
      "Epoch 16: Batch 128, LR: 0.100000, Train Acc: 0.8782, Val Acc: 0.9021\n",
      "Epoch 17: Batch 128, LR: 0.100000, Train Acc: 0.8928, Val Acc: 0.8637\n",
      "Epoch 18: Batch 128, LR: 0.100000, Train Acc: 0.8281, Val Acc: 0.8503\n",
      "Epoch 19: Batch 128, LR: 0.100000, Train Acc: 0.8497, Val Acc: 0.8004\n",
      "Epoch 20: Batch 128, LR: 0.100000, Train Acc: 0.8357, Val Acc: 0.9175\n",
      "✅ Best model saved (Batch 128) with val_acc 0.9175\n",
      "Epoch 21: Batch 128, LR: 0.100000, Train Acc: 0.8730, Val Acc: 0.9155\n",
      "Epoch 22: Batch 128, LR: 0.100000, Train Acc: 0.8543, Val Acc: 0.8081\n",
      "Epoch 23: Batch 128, LR: 0.100000, Train Acc: 0.8555, Val Acc: 0.8695\n",
      "Epoch 24: Batch 128, LR: 0.100000, Train Acc: 0.8823, Val Acc: 0.8157\n",
      "Epoch 25: Batch 128, LR: 0.100000, Train Acc: 0.8759, Val Acc: 0.9405\n",
      "✅ Best model saved (Batch 128) with val_acc 0.9405\n",
      "Epoch 26: Batch 128, LR: 0.100000, Train Acc: 0.8765, Val Acc: 0.9079\n",
      "Epoch 27: Batch 128, LR: 0.100000, Train Acc: 0.9056, Val Acc: 0.9079\n",
      "Epoch 28: Batch 128, LR: 0.100000, Train Acc: 0.9237, Val Acc: 0.9002\n",
      "Epoch 29: Batch 128, LR: 0.100000, Train Acc: 0.8916, Val Acc: 0.9117\n",
      "Epoch 30: Batch 128, LR: 0.100000, Train Acc: 0.9307, Val Acc: 0.9271\n",
      "Epoch 31: Batch 128, LR: 0.100000, Train Acc: 0.8537, Val Acc: 0.8656\n",
      "Epoch 32: Batch 128, LR: 0.100000, Train Acc: 0.8526, Val Acc: 0.8964\n",
      "Epoch 33: Batch 128, LR: 0.100000, Train Acc: 0.8899, Val Acc: 0.4952\n",
      "Epoch 34: Batch 128, LR: 0.100000, Train Acc: 0.8776, Val Acc: 0.8464\n",
      "Epoch 35: Batch 128, LR: 0.100000, Train Acc: 0.8671, Val Acc: 0.8656\n",
      "Epoch 36: Batch 128, LR: 0.100000, Train Acc: 0.8071, Val Acc: 0.8503\n",
      "Epoch 37: Batch 128, LR: 0.100000, Train Acc: 0.8339, Val Acc: 0.8042\n",
      "Epoch 38: Batch 128, LR: 0.100000, Train Acc: 0.8800, Val Acc: 0.9002\n",
      "Epoch 39: Batch 128, LR: 0.100000, Train Acc: 0.8596, Val Acc: 0.8964\n",
      "Epoch 40: Batch 128, LR: 0.100000, Train Acc: 0.8549, Val Acc: 0.9213\n",
      "Epoch 41: Batch 128, LR: 0.010000, Train Acc: 0.8893, Val Acc: 0.8656\n",
      "Epoch 42: Batch 128, LR: 0.010000, Train Acc: 0.9027, Val Acc: 0.9424\n",
      "✅ Best model saved (Batch 128) with val_acc 0.9424\n",
      "Epoch 43: Batch 128, LR: 0.010000, Train Acc: 0.9248, Val Acc: 0.9367\n",
      "Epoch 44: Batch 128, LR: 0.010000, Train Acc: 0.8992, Val Acc: 0.9309\n",
      "Epoch 45: Batch 128, LR: 0.010000, Train Acc: 0.9050, Val Acc: 0.9309\n",
      "Epoch 46: Batch 128, LR: 0.010000, Train Acc: 0.9213, Val Acc: 0.9175\n",
      "Epoch 47: Batch 128, LR: 0.010000, Train Acc: 0.9376, Val Acc: 0.9347\n",
      "Epoch 48: Batch 128, LR: 0.010000, Train Acc: 0.9178, Val Acc: 0.9213\n",
      "Epoch 49: Batch 128, LR: 0.010000, Train Acc: 0.8992, Val Acc: 0.9443\n",
      "✅ Best model saved (Batch 128) with val_acc 0.9443\n",
      "Epoch 50: Batch 128, LR: 0.010000, Train Acc: 0.9213, Val Acc: 0.9482\n",
      "✅ Best model saved (Batch 128) with val_acc 0.9482\n",
      "Epoch 51: Batch 128, LR: 0.010000, Train Acc: 0.9242, Val Acc: 0.9347\n",
      "Epoch 52: Batch 128, LR: 0.010000, Train Acc: 0.9481, Val Acc: 0.9347\n",
      "Epoch 53: Batch 128, LR: 0.010000, Train Acc: 0.9178, Val Acc: 0.9271\n",
      "Epoch 54: Batch 128, LR: 0.010000, Train Acc: 0.9266, Val Acc: 0.9232\n",
      "Epoch 55: Batch 128, LR: 0.010000, Train Acc: 0.9272, Val Acc: 0.9386\n",
      "Epoch 56: Batch 128, LR: 0.010000, Train Acc: 0.9336, Val Acc: 0.9232\n",
      "Epoch 57: Batch 128, LR: 0.001000, Train Acc: 0.9458, Val Acc: 0.9386\n",
      "Epoch 58: Batch 128, LR: 0.001000, Train Acc: 0.9400, Val Acc: 0.9386\n",
      "Epoch 59: Batch 128, LR: 0.001000, Train Acc: 0.9143, Val Acc: 0.9386\n",
      "Epoch 60: Batch 128, LR: 0.001000, Train Acc: 0.9400, Val Acc: 0.9405\n",
      "Epoch 61: Batch 128, LR: 0.001000, Train Acc: 0.9009, Val Acc: 0.9290\n",
      "Epoch 62: Batch 128, LR: 0.001000, Train Acc: 0.9307, Val Acc: 0.9328\n",
      "Epoch 63: Batch 128, LR: 0.001000, Train Acc: 0.9353, Val Acc: 0.9347\n",
      "Epoch 64: Batch 128, LR: 0.001000, Train Acc: 0.9324, Val Acc: 0.9232\n",
      "Epoch 65: Batch 128, LR: 0.001000, Train Acc: 0.9441, Val Acc: 0.9386\n",
      "Epoch 66: Batch 128, LR: 0.001000, Train Acc: 0.9330, Val Acc: 0.9386\n",
      "Epoch 67: Batch 128, LR: 0.001000, Train Acc: 0.9382, Val Acc: 0.9405\n",
      "Epoch 68: Batch 128, LR: 0.001000, Train Acc: 0.9435, Val Acc: 0.9405\n",
      "Epoch 69: Batch 128, LR: 0.001000, Train Acc: 0.9411, Val Acc: 0.9405\n",
      "Epoch 70: Batch 128, LR: 0.001000, Train Acc: 0.9307, Val Acc: 0.9271\n",
      "Epoch 71: Batch 128, LR: 0.001000, Train Acc: 0.9079, Val Acc: 0.9405\n",
      "Epoch 72: Batch 128, LR: 0.001000, Train Acc: 0.9318, Val Acc: 0.9405\n",
      "Epoch 73: Batch 128, LR: 0.000100, Train Acc: 0.9411, Val Acc: 0.9290\n",
      "Epoch 74: Batch 128, LR: 0.000100, Train Acc: 0.9318, Val Acc: 0.9213\n",
      "Epoch 75: Batch 128, LR: 0.000100, Train Acc: 0.9114, Val Acc: 0.9271\n",
      "Epoch 76: Batch 128, LR: 0.000100, Train Acc: 0.9470, Val Acc: 0.9232\n",
      "Epoch 77: Batch 128, LR: 0.000100, Train Acc: 0.9417, Val Acc: 0.9424\n",
      "Epoch 78: Batch 128, LR: 0.000100, Train Acc: 0.9476, Val Acc: 0.9405\n",
      "Epoch 79: Batch 128, LR: 0.000100, Train Acc: 0.9411, Val Acc: 0.9290\n",
      "Epoch 80: Batch 128, LR: 0.000100, Train Acc: 0.9283, Val Acc: 0.9424\n",
      "🛑 Early stopping at epoch 80\n",
      "\n",
      "✅ Training completed! Best model: best_overall_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Define paths\n",
    "base_path = r\"C:\\Users\\ChoiSeongHyeon\\Desktop\\Dataset\\CWRU\\Data Division\"\n",
    "\n",
    "# Fault type labels\n",
    "fault_types = {'N': 0, 'IR': 1, 'OR@06': 2, 'B': 3}\n",
    "\n",
    "# Load dataset\n",
    "class CWRUDataset(Dataset):\n",
    "    def __init__(self, dataset_type=\"Train\"):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for fault, label in fault_types.items():\n",
    "            sample_path = os.path.join(base_path, fault, \"Samples\")\n",
    "\n",
    "            if not os.path.exists(sample_path):\n",
    "                print(f\"Warning: Path does not exist - {sample_path}\")\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(sample_path):\n",
    "                if file.endswith('.csv') and dataset_type in file:\n",
    "                    file_path = os.path.join(sample_path, file)\n",
    "                    data = pd.read_csv(file_path, header=None).values.flatten()\n",
    "\n",
    "                    if len(data) == 4096:\n",
    "                        self.data.append(data)\n",
    "                        self.labels.append(label)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32).unsqueeze(1)  # (N, 1, 4096)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "        # Validation & Test 데이터는 한 번만 섞기 (재현 가능하도록)\n",
    "        if dataset_type in [\"Validation\", \"Test\"]:\n",
    "            np.random.seed(42)\n",
    "            indices = np.random.permutation(len(self.data))\n",
    "            self.data = self.data[indices]\n",
    "            self.labels = self.labels[indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Define WDCNN Model\n",
    "class WDCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(WDCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=64, stride=16, padding=24)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(64, 100)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.global_max_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Training function with dynamic batch size\n",
    "def train_model(train_dataset, val_dataset, batch_sizes=[32, 64, 128], num_epochs=300, learning_rate=0.1, patience=30):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    best_overall_acc = 0.0\n",
    "    best_model_path = \"best_overall_model.pth\"\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\n🔹 Training with Batch Size: {batch_size}\")\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = WDCNN().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=15, verbose=True)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss, train_correct = 0.0, 0\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss, val_correct = 0.0, 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            val_acc = val_correct / len(val_loader.dataset)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}: Batch {batch_size}, LR: {current_lr:.6f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Save best model for current batch size\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                early_stop_counter = 0\n",
    "                torch.save(model.state_dict(), f\"best_model_bs{batch_size}.pth\")\n",
    "                print(f\"✅ Best model saved (Batch {batch_size}) with val_acc {best_val_acc:.4f}\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"🛑 Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Save best overall model\n",
    "        if best_val_acc > best_overall_acc:\n",
    "            best_overall_acc = best_val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"\\n🏆 New Best Overall Model Saved! Val Acc: {best_overall_acc:.4f} (Batch {batch_size})\")\n",
    "\n",
    "    print(\"\\n✅ Training completed! Best model:\", best_model_path)\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_dataset = CWRUDataset(\"Train\")\n",
    "val_dataset = CWRUDataset(\"Validation\")\n",
    "\n",
    "# Train with dynamic batch size\n",
    "train_model(train_dataset, val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.1 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChoiSeongHyeon\\AppData\\Local\\Temp\\ipykernel_35748\\919866871.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"best_overall_model.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.99      1.00       160\n",
      "          IR       0.92      0.91      0.91       121\n",
      "       OR@06       0.87      0.93      0.90       120\n",
      "           B       0.94      0.90      0.92       120\n",
      "\n",
      "    accuracy                           0.94       521\n",
      "   macro avg       0.93      0.93      0.93       521\n",
      "weighted avg       0.94      0.94      0.94       521\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 🔹 Function to Evaluate Model on Test Set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # 🔹 Print Classification Report\n",
    "    print(\"\\n📌 Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=fault_types.keys()))\n",
    "\n",
    "    # 🔹 Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=fault_types.keys(), yticklabels=fault_types.keys())\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 🔹 Initialize Trained Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WDCNN().to(device)\n",
    "\n",
    "# 🔹 Load Best Model with Error Handling\n",
    "checkpoint = torch.load(\"best_overall_model.pth\", map_location=device)\n",
    "\n",
    "if \"model_state_dict\" in checkpoint:\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "# 🔹 Load Test Dataset\n",
    "test_dataset = CWRUDataset(\"Test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # 🔹 최적의 Batch Size 적용\n",
    "\n",
    "# 🔹 Evaluate Model on Test Dataset\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4. Unstructured Pruning 적용 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4.1 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5. Structured Pruning 적용 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5.1 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6. Winning Ticket 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6.x 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 7. 최종 성능 비교"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
