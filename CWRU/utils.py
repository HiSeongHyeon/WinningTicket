import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.utils.prune as prune
import time
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from copy import deepcopy
from sklearn.metrics import classification_report, confusion_matrix
from model import WDCNN
import pickle
import random
from IPython.display import display

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # ë‹¤ì¤‘ GPU ì‚¬ìš© ì‹œ í•„ìš”
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# ============================================================
# ğŸ”¹ íŒŒë¼ë¯¸í„° ì €ì¥ ê²½ë¡œ ì •ì˜
# ============================================================
parameter_dir = r"C:\Users\ChoiSeongHyeon\Desktop\WinningT\Winning Ticket\MyWinningTicket\CWRU\Parameters"
original_path = os.path.join(parameter_dir, "Original")
unstructured_path = os.path.join(parameter_dir, "Unstructured")
structured_path = os.path.join(parameter_dir, "Structured")

unstructured_results_path = os.path.join(unstructured_path, "unstructured_experiment_results.pkl")
structured_results_path = os.path.join(structured_path, "structured_experiment_results.pkl")

# í´ë” ìƒì„±
os.makedirs(original_path, exist_ok=True)
os.makedirs(unstructured_path, exist_ok=True)
os.makedirs(structured_path, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================================================
# ğŸ”¹ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ============================================================
class CWRUDataset(Dataset):
    def __init__(self, dataset_type):
        base_path = r"C:\Users\ChoiSeongHyeon\Desktop\Dataset\CWRU\Data Division"
        fault_types = {'N': 0, 'IR': 1, 'OR@06': 2, 'B': 3}

        self.data, self.labels = [], []

        for fault, label in fault_types.items():
            sample_path = os.path.join(base_path, fault, "Samples")
            if not os.path.exists(sample_path):
                print(f"Warning: Path does not exist - {sample_path}")
                continue

            for file in os.listdir(sample_path):
                if file.endswith('.csv') and dataset_type in file:
                    file_path = os.path.join(sample_path, file)
                    data = pd.read_csv(file_path, header=None).values.flatten()
                    # Data Sample Length
                    if len(data) == 4096:
                        self.data.append(data)
                        self.labels.append(label)

        # Convert to Pytorch Tensors
        self.data = torch.tensor(self.data, dtype=torch.float32).unsqueeze(1)
        self.labels = torch.tensor(self.labels, dtype=torch.long)

        # Validation & Test ë°ì´í„°ëŠ” í•œ ë²ˆë§Œ ì„ê¸° (ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡)
        if dataset_type in ["Validation", "Test"]:
            np.random.seed(42)
            indices = np.random.permutation(len(self.data))
            self.data = self.data[indices]
            self.labels = self.labels[indices]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]
    
# ============================================================
# ğŸ”¹ Original Model í•™ìŠµ í•¨ìˆ˜ (ë‹¤ì–‘í•œ Batch Size)
# ============================================================
def train_original_model_with_batch_sizes(train_dataset, val_dataset, batch_sizes=[256], num_epochs=500, learning_rate=0.01, patience=200):
    """Original Modelì„ ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í•™ìŠµí•˜ê³  best_overall_model.pthì— ì €ì¥"""
    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    best_overall_acc = 0.0
    best_batch_size = None
    best_model_path = os.path.join(original_path, "best_overall_model.pth")
    best_initial_weights_path = os.path.join(original_path, "best_initial_weights.pth")

    for batch_size in batch_sizes:
        print(f"\nğŸ”¹ Training Original Model with Batch Size: {batch_size}")

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42))
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        
        set_seed(42)
        # ëª¨ë¸ì˜ ì´ˆê¸°ê°’ì´ í• ë‹¹
        model = WDCNN().to(device)

        # ğŸ”¹ **í˜„ì¬ ë°°ì¹˜ í¬ê¸°ì˜ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥**
        init_weights_path = os.path.join(original_path, f"initial_weights_bs{batch_size}.pth")
        torch.save(model.state_dict(), init_weights_path)
        print(f"âœ… Initial weights saved for batch {batch_size}!")

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)  

        best_val_acc = 0.0
        early_stop_counter = 0

        for epoch in range(num_epochs):
            model.train()
            train_loss, train_correct = 0.0, 0

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()
                train_correct += (outputs.argmax(dim=1) == labels).sum().item()

            train_acc = train_correct / len(train_loader.dataset)

            # ğŸ”¹ Validation
            model.eval()
            val_loss, val_correct = 0.0, 0
            with torch.no_grad():
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item()
                    val_correct += (outputs.argmax(dim=1) == labels).sum().item()

            val_acc = val_correct / len(val_loader.dataset)
            scheduler.step(val_loss)

            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch+1}: Batch {batch_size}, LR: {current_lr:.6f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

            # Save best model for current batch size
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                early_stop_counter = 0
                torch.save(model.state_dict(), os.path.join(original_path, f"best_model_bs{batch_size}.pth"))
                print(f"âœ… Best model saved (Batch {batch_size}) with val_acc {best_val_acc:.4f}")
            else:
                early_stop_counter += 1

            if early_stop_counter >= patience:
                print(f"ğŸ›‘ Early stopping at epoch {epoch+1}")
                break

        # ğŸ”¹ Save best overall model and corresponding initial weights
        if best_val_acc > best_overall_acc:
            best_overall_acc = best_val_acc
            best_batch_size = batch_size
            torch.save(model.state_dict(), best_model_path)
            best_initial_weights_path = init_weights_path  # ğŸ”¹ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ ì´ˆê¸° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            print(f"\nğŸ† New Best Overall Model Saved! Val Acc: {best_overall_acc:.4f} (Batch {best_batch_size})")

    print(f"\nâœ… Training completed! Best model: {best_model_path} from batch size {best_batch_size}")
    print(f"ğŸ¯ Best Initial Weights Saved: {best_initial_weights_path}")

# ============================================================
# ğŸ”¹ Classification Report ë° Confusion Matrix
# ============================================================
def evaluate_classification(model, test_loader, device, title="Model Evaluation"):
    """Test Datasetì—ì„œ Classification Report ë° Confusion Matrix ì¶œë ¥"""
    model.to(device)
    model.eval()
    
    y_true, y_pred = [], []
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            predictions = torch.argmax(outputs, dim=1)
            
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predictions.cpu().numpy())

    print(f"\nğŸ“Œ {title} - Classification Report:")
    print(classification_report(y_true, y_pred, target_names=['N', 'IR', 'OR@06', 'B']))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['N', 'IR', 'OR@06', 'B'], yticklabels=['N', 'IR', 'OR@06', 'B'])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"{title} - Confusion Matrix")
    plt.show()

# ============================================================
# ğŸ”¹ Pruning í›„ ì €ì¥ í•¨ìˆ˜
# ============================================================
def remove_pruning_and_save_unstructured(model, save_path):
    """Unstructured Pruningëœ ëª¨ë¸ì—ì„œ Pruning Maskë¥¼ ì œê±°í•˜ê³  ì €ì¥"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):
            prune.remove(module, "weight")  # âœ… Unstructured Pruning ì œê±°
    torch.save(model.state_dict(), save_path)  # âœ… ê°€ì¤‘ì¹˜ ì €ì¥ (Pruning Mask ì œê±°ë¨)

def remove_pruning_and_save_structured(model, save_path):
    """Structured Pruningëœ ëª¨ë¸ì—ì„œ Pruning Maskë¥¼ ì œê±°í•˜ê³  ì €ì¥"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv1d):  # âœ… Structured Pruningëœ Conv1d Layerë§Œ ì ìš©
            prune.remove(module, "weight")  # âœ… Pruning Mask ì œê±°
    torch.save(model.state_dict(), save_path)  # âœ… Pruning Mask ì—†ì´ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥

# ============================================================
# ğŸ”¹ Unstructured Pruning Fine-Tuning ìˆ˜í–‰ ë° ì €ì¥ (í›ˆë ¨ 1ë²ˆë§Œ ìˆ˜í–‰)
# ============================================================
def apply_sparse_training(model, amount):
    """Unstructured Pruningì„ ëª¨ë¸ì— ì ìš©"""
    model = deepcopy(model)
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):
            prune.l1_unstructured(module, name="weight", amount=amount)
    return model

def fine_tune_unstructured(pruning_amounts, train_loader, val_loader, num_epochs=100, save_results=True):
    """Unstructured Pruning Fine-Tuning ìˆ˜í–‰ í›„ ê²°ê³¼ë¥¼ ì €ì¥"""
    
    # âœ… ê¸°ì¡´ í•™ìŠµ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸° (ë¶ˆí•„ìš”í•œ í•™ìŠµ ë°©ì§€)
    if os.path.exists(unstructured_results_path):
        with open(unstructured_results_path, "rb") as f:
            results = pickle.load(f)
        print("\nâœ… ê¸°ì¡´ Fine-Tuning ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return results
    
    # âœ… ìƒˆë¡­ê²Œ Fine-Tuning ì‹¤í–‰
    results = {}

    for pruning_amount in pruning_amounts:
        print(f"\nğŸ”¹ Fine-tuning with Unstructured Pruning Amount: {pruning_amount}")

        model = WDCNN().to(device)
        model.load_state_dict(torch.load(original_path + "/best_overall_model.pth", map_location=device))
        pruned_model = apply_sparse_training(model, amount=pruning_amount)
        pruned_model.to(device)

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(pruned_model.parameters(), lr=0.00001)

        best_val_acc = 0.0
        best_pruned_model = None
        val_accuracies = []

        for epoch in range(num_epochs):
            pruned_model.train()
            train_correct = 0

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = pruned_model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                train_correct += (outputs.argmax(dim=1) == labels).sum().item()

            train_acc = train_correct / len(train_loader.dataset)

            # Validation
            pruned_model.eval()
            val_correct = 0
            with torch.no_grad():
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = pruned_model(inputs)
                    val_correct += (outputs.argmax(dim=1) == labels).sum().item()

            val_acc = val_correct / len(val_loader.dataset)
            val_accuracies.append(val_acc)

            # ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ ì¶œë ¥
            print(f"Epoch {epoch+1}/{num_epochs} - Pruning {pruning_amount:.1f}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_pruned_model = deepcopy(pruned_model)

        # âœ… ìµœì  ëª¨ë¸ ì €ì¥
        model_save_path = os.path.join(unstructured_path, f"best_unstructured_{int(pruning_amount*100)}.pth")
        remove_pruning_and_save_unstructured(best_pruned_model, model_save_path)

        results[pruning_amount] = {
            "Best Validation Accuracy": best_val_acc,
            "Validation Accuracy per Epoch": val_accuracies
        }

    # âœ… Fine-Tuning ê²°ê³¼ ì €ì¥
    if save_results:
        with open(unstructured_results_path, "wb") as f:
            pickle.dump(results, f)
        print("\nâœ… Fine-Tuning Results Saved!")

    return results

def load_unstructured_results():
    """ì €ì¥ëœ Unstructured Pruning ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°"""
    if os.path.exists(unstructured_results_path):
        with open(unstructured_results_path, "rb") as f:
            results = pickle.load(f)
        print("\nâœ… Loaded Saved Unstructured Pruning Results!")
        return results
    else:
        print("\nâŒ No Saved Results Found. Need to Fine-Tune First!")
        return None

def plot_unstructured_pruning_results(experiment_results, num_epochs, val_loader, include_original=True):
    """Pruningëœ ëª¨ë¸ë“¤ì˜ Validation Accuracy ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ì¶œë ¥ (Original Model í¬í•¨ ê°€ëŠ¥)"""
    plt.figure(figsize=(10, 6))

    # âœ… Original Model Validation Accuracy ì¶”ê°€
    if include_original:
        original_model = WDCNN().to(device)
        original_model.load_state_dict(torch.load(original_path + "/best_overall_model.pth", map_location=device))
        original_model.eval()
        
        val_accuracies_original = []
        with torch.no_grad():
            for epoch in range(1, num_epochs + 1):
                val_correct = 0
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = original_model(inputs)
                    val_correct += (outputs.argmax(dim=1) == labels).sum().item()

                val_acc = val_correct / len(val_loader.dataset)
                val_accuracies_original.append(val_acc)

        # âœ… Original Model Plot (Pruning 0%)
        plt.plot(range(1, num_epochs+1), val_accuracies_original, label="Original Model (Pruning Ratio 0%)", linestyle="--", color="black")

    # âœ… Pruned Models Plot
    for pruning_ratio, result in experiment_results.items():
        val_accuracies = result["Validation Accuracy per Epoch"]
        if len(val_accuracies) > 0:
            plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=f"Pruning Ratio {pruning_ratio*100:.0f}%")

    plt.xlabel("Training Epoch")
    plt.ylabel("Validation Accuracy")
    plt.title("Validation Accuracy of WDCNN \nwith Different Unstructured Pruning Ratios")
    plt.legend()
    plt.grid(True)
    plt.show()

def evaluate_test_performance_unstructured(original_model, test_loader, device, experiment_results, model_save_path):
    """Original ë° Unstructured Pruned Modelì˜ Test Accuracy, Inference Timeì„ í‰ê°€ (í…Œì´ë¸” ì¶œë ¥)"""
    
    test_accuracies = []

    # âœ… Original Model ì„±ëŠ¥ í‰ê°€
    original_model.to(device)
    original_model.eval()

    total_params = count_total_parameters(original_model)
    test_correct = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = original_model(inputs)
            test_correct += (outputs.argmax(dim=1) == labels).sum().item()

    original_test_acc = test_correct / len(test_loader.dataset)
    original_inference_time = measure_inference_time(original_model, test_loader, device)

    test_accuracies.append({
        "Pruning Ratio": 0.0,
        "Number of Non-Zero Params": total_params,
        "Inference Time (s)": original_inference_time,
        "Test Accuracy": original_test_acc
    })

    # ğŸ”¹ Unstructured Pruned Models ì„±ëŠ¥ í‰ê°€
    for pruning_ratio in experiment_results.keys():
        model_path = os.path.join(model_save_path, f"best_unstructured_{int(pruning_ratio*100)}.pth")  # âœ… Unstructured Pruning ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

        if not os.path.exists(model_path):
            print(f"âŒ Model file not found for Pruning Ratio: {pruning_ratio}")
            continue

        # ğŸ”¹ Best Pruned Model ë¡œë“œ
        model = WDCNN().to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))

        nonzero_params = count_nonzero_parameters(model)

        # ğŸ”¹ Test Accuracy ì¸¡ì •
        model.eval()
        test_correct = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                test_correct += (outputs.argmax(dim=1) == labels).sum().item()

        test_acc = test_correct / len(test_loader.dataset)

        # ğŸ”¹ Inference Time ì¸¡ì •
        inference_time = measure_inference_time(model, test_loader, device)

        # ğŸ”¹ ê²°ê³¼ ì €ì¥
        test_accuracies.append({
            "Pruning Ratio": pruning_ratio,
            "Number of Non-Zero Params": nonzero_params,
            "Inference Time (s)": inference_time,
            "Test Accuracy": test_acc
        })

    return pd.DataFrame(test_accuracies)  # âœ… ìµœì¢… ê²°ê³¼ í…Œì´ë¸” ë°˜í™˜

# ============================================================
# ğŸ”¹ Structured Pruning Fine-Tuning ìˆ˜í–‰ ë° ì €ì¥ (í›ˆë ¨ 1ë²ˆë§Œ ìˆ˜í–‰)
# ============================================================
def apply_structured_pruning(model, amount):
    model = deepcopy(model)  # âœ… ëª¨ë¸ ë³µì‚¬ í›„ Pruning ì ìš©
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv1d):  # âœ… Linear Layer ì œì™¸
            prune.ln_structured(module, name="weight", amount=amount, n=2, dim=0)
    return model  # âœ… Prunedëœ ëª¨ë¸ ë°˜í™˜

def calculate_actual_sparsity(model):
    """ì‹¤ì œ Structured Pruning Ratio ê³„ì‚° (ë¹„-ì œë¡œ ê°€ì¤‘ì¹˜ ë¹„ìœ¨)"""
    total_weights = 0
    zero_weights = 0
    for name, module in model.named_modules():
        if isinstance(module, nn.Conv1d):
            total_weights += module.weight.nelement()
            zero_weights += (module.weight == 0).sum().item()
    return zero_weights / total_weights if total_weights > 0 else 0.0

def fine_tune_structured(pruning_amounts, train_loader, val_loader, num_epochs=100, save_results=True):
    """Structured Pruning Fine-Tuningì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥"""
    
    # âœ… ê¸°ì¡´ í•™ìŠµ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸° (ë¶ˆí•„ìš”í•œ í•™ìŠµ ë°©ì§€)
    if os.path.exists(structured_results_path):
        with open(structured_results_path, "rb") as f:
            results = pickle.load(f)
        print("\nâœ… ê¸°ì¡´ Structured Fine-Tuning ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return results

    # âœ… ìƒˆë¡­ê²Œ Fine-Tuning ì‹¤í–‰
    results = {}

    for pruning_amount in pruning_amounts:
        print(f"\nğŸ”¹ Fine-tuning with Structured Pruning Amount: {pruning_amount}")

        model = WDCNN().to(device)
        model.load_state_dict(torch.load(original_path + "/best_overall_model.pth", map_location=device))
        pruned_model = apply_structured_pruning(model, amount=pruning_amount)  # âœ… Pruned ëª¨ë¸ ë°˜í™˜
        pruned_model.to(device)

        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(pruned_model.parameters(), lr=0.00001)

        best_val_acc = 0.0
        best_pruned_model = None
        val_accuracies = []

        for epoch in range(num_epochs):
            pruned_model.train()
            train_correct = 0

            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = pruned_model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                train_correct += (outputs.argmax(dim=1) == labels).sum().item()

            train_acc = train_correct / len(train_loader.dataset)

            # Validation
            pruned_model.eval()
            val_correct = 0
            with torch.no_grad():
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = pruned_model(inputs)
                    val_correct += (outputs.argmax(dim=1) == labels).sum().item()

            val_acc = val_correct / len(val_loader.dataset)
            val_accuracies.append(val_acc)

            print(f"Epoch {epoch+1}/{num_epochs} - Pruning {pruning_amount:.1f}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_pruned_model = deepcopy(pruned_model)

        # âœ… ìµœì  ëª¨ë¸ ì €ì¥ (Pruning Mask ì œê±° í›„ ì €ì¥)
        model_save_path = os.path.join(structured_path, f"best_structured_finetuned_{int(pruning_amount*100)}.pth")
        remove_pruning_and_save_structured(best_pruned_model, model_save_path)

        results[pruning_amount] = {
            "Pruning Ratio": pruning_amount,
            "Best Validation Accuracy": best_val_acc,
            "Validation Accuracy per Epoch": val_accuracies
        }

    # âœ… Fine-Tuning ê²°ê³¼ ì €ì¥
    if save_results:
        with open(structured_results_path, "wb") as f:
            pickle.dump(results, f)
        print("\nâœ… Structured Fine-Tuning Results Saved!")

    return results

def plot_structured_pruning_results(experiment_results, num_epochs, val_loader, include_original=False):
    """Structured Pruningëœ ëª¨ë¸ë“¤ì˜ Validation Accuracy ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ì¶œë ¥"""

    plt.figure(figsize=(10, 6))

    # âœ… Original Model Plot (Pruning 0%) ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
    if include_original:
        print("\nğŸ”¹ Calculating Original Model Validation Accuracy for Graph...")
        original_model = WDCNN().to(device)
        original_model.load_state_dict(torch.load(original_path + "/best_overall_model.pth", map_location=device))
        original_model.eval()

        val_accuracies_original = []
        with torch.no_grad():
            for epoch in range(1, num_epochs + 1):  
                val_correct = 0
                for inputs, labels in val_loader:
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = original_model(inputs)
                    val_correct += (outputs.argmax(dim=1) == labels).sum().item()

                val_acc = val_correct / len(val_loader.dataset)
                val_accuracies_original.append(val_acc)

        plt.plot(range(1, num_epochs + 1), val_accuracies_original, 
                 label="Original Model (Pruning Ratio 0%)", linestyle="--", color="black")

    # âœ… Structured Pruned Models Plot
    for pruning_ratio, result in experiment_results.items():
        val_accuracies = result["Validation Accuracy per Epoch"]
        if len(val_accuracies) > 0:
            plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, 
                     label=f"Structured Pruning {pruning_ratio*100:.0f}%")

    plt.xlabel("Training Epoch")
    plt.ylabel("Validation Accuracy")
    plt.title("Validation Accuracy of WDCNN \nwith Different Structured Pruning Ratios")
    plt.legend()
    plt.grid(True)
    plt.show()


def load_structured_results():
    """ì €ì¥ëœ Structured Pruning ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°"""
    if os.path.exists(structured_results_path):
        with open(structured_results_path, "rb") as f:
            results = pickle.load(f)
        print("\nâœ… Loaded Saved Structured Pruning Results!")
        return results
    else:
        print("\nâŒ No Saved Results Found. Need to Fine-Tune First!")
        return None

def evaluate_test_performance_structured(original_model, test_loader, device, experiment_results, model_save_path):
    """Original ë° Structured Pruned Modelì˜ Test Accuracy, Inference Timeì„ í‰ê°€ (í…Œì´ë¸” ì¶œë ¥)"""
    
    test_accuracies = []

    # âœ… Original Model ì„±ëŠ¥ í‰ê°€
    original_model.to(device)
    original_model.eval()

    total_params = count_total_parameters(original_model)
    test_correct = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = original_model(inputs)
            test_correct += (outputs.argmax(dim=1) == labels).sum().item()

    original_test_acc = test_correct / len(test_loader.dataset)
    original_inference_time = measure_inference_time(original_model, test_loader, device)

    test_accuracies.append({
        "Pruning Ratio": 0.0,
        "Number of Non-Zero Params": total_params,
        "Inference Time (s)": original_inference_time,
        "Test Accuracy": original_test_acc
    })

    # ğŸ”¹ Structured Pruned Models ì„±ëŠ¥ í‰ê°€
    for pruning_ratio in experiment_results.keys():
        model_path = os.path.join(model_save_path, f"best_structured_finetuned_{int(pruning_ratio*100)}.pth")  # âœ… Structured Pruning ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°

        if not os.path.exists(model_path):
            print(f"âŒ Model file not found for Pruning Ratio: {pruning_ratio}")
            continue

        # ğŸ”¹ Best Pruned Model ë¡œë“œ
        model = WDCNN().to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))

        nonzero_params = count_nonzero_parameters(model)

        # ğŸ”¹ Test Accuracy ì¸¡ì •
        model.eval()
        test_correct = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                test_correct += (outputs.argmax(dim=1) == labels).sum().item()

        test_acc = test_correct / len(test_loader.dataset)

        # ğŸ”¹ Inference Time ì¸¡ì •
        inference_time = measure_inference_time(model, test_loader, device)

        # ğŸ”¹ ê²°ê³¼ ì €ì¥
        test_accuracies.append({
            "Pruning Ratio": pruning_ratio,
            "Number of Non-Zero Params": nonzero_params,
            "Inference Time (s)": inference_time,
            "Test Accuracy": test_acc
        })

    return pd.DataFrame(test_accuracies)  # âœ… ìµœì¢… ê²°ê³¼ í…Œì´ë¸” ë°˜í™˜

# ğŸ”¹ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def count_total_parameters(model):
    """ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°"""
    return sum(p.numel() for p in model.parameters())

# ğŸ”¹ ë¹„-ì œë¡œ ê°€ì¤‘ì¹˜ ê°œìˆ˜ ê³„ì‚° í•¨ìˆ˜
def count_nonzero_parameters(model):
    """Pruningëœ ëª¨ë¸ì˜ ë¹„-ì œë¡œ ê°€ì¤‘ì¹˜ ê°œìˆ˜ ê³„ì‚°"""
    return sum((p != 0).sum().item() for p in model.parameters() if p.requires_grad)

# ğŸ”¹ Inference Time ì¸¡ì • í•¨ìˆ˜ (100ê°œ ìƒ˜í”Œ í‰ê· )
def measure_inference_time(model, test_loader, device, num_samples=100):
    """ëª¨ë¸ì˜ í‰ê·  Inference Time ì¸¡ì •"""
    model.eval()
    total_time = 0.0
    num_runs = 0

    with torch.no_grad():
        for inputs, _ in test_loader:
            if num_runs >= num_samples:
                break
            inputs = inputs.to(device)

            # ì‹œê°„ ì¸¡ì • ì‹œì‘
            start_time = time.time()
            _ = model(inputs)
            end_time = time.time()

            total_time += (end_time - start_time)
            num_runs += 1

    return total_time / num_runs if num_runs > 0 else 0.0  # í‰ê·  Inference Time (ì´ˆ)